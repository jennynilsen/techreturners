[
{
	"uri": "/03-pods/01-what-is-pod/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " What is a Pod ? A Pod is the basic building block of Kubernetes–the smallest and simplest unit in the Kubernetes object model that you create or deploy. A Pod represents a running process on your cluster\nThe “one-container-per-Pod” model is the most common Kubernetes use case; in this case, you can think of a Pod as a wrapper around a single container, and Kubernetes manages the Pods rather than the containers directly.\nA Pod might encapsulate an application composed of multiple co-located containers that are tightly coupled and need to share resources. These co-located containers might form a single cohesive unit of service–one container serving files from a shared volume to the public, while a separate “sidecar” container refreshes or updates those files. The Pod wraps these containers and storage resources together as a single manageable entity.\nWhat is a Node? A Pod always runs on a Node. A Node is a worker machine in Kubernetes and may be either a virtual or a physical machine, depending on the cluster. Each Node is managed by the Master. A Node can have multiple pods, and the Kubernetes master automatically handles scheduling the pods across the Nodes in the cluster. The Master\u0026rsquo;s automatic scheduling takes into account the available resources on each Node.\n"
},
{
	"uri": "/04-labels_and_annotations/01-labels/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Why we need labels ? If you have a bucket of white dominos and you want to group it based on the number of dots.\nLets say we want all dominos with 10 dots; we will take domino one by one and if its having 10 dots ,we will put it aside and continue the same operation until all dominos were checked.\nLikewise , suppose if you have 100 pods and few of them are nginx and few of them are centos , how we can see only nginx pods ?\nWe need a label on each pod so that we can tell kubectl command to show the pods with that label.\nIn kubernetes , label is a key value pair and it provides \u0026lsquo;identifying metadata\u0026rsquo; for objects. These are fundamental qualities of objects that will be used for grouping , viewing and operating.\nFor now we will se how we can view them (Will discuss about grouping and operation on pod groups later)\nPod labels Lets run a Coffee app Pod\nk8s@k8s-master-01:~$ kubectl run coffee-app --image=ansilh/demo-coffee --restart=Never pod/coffee-app created k8s@k8s-master-01:~$ kubectl get pods NAME READY STATUS RESTARTS AGE coffee-app 1/1 Running 0 4s k8s@k8s-master-01:~$  See the labels of a Pods k8s@k8s-master-01:~$ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS coffee-app 1/1 Running 0 37s run=coffee-app k8s@k8s-master-01:~$  As you can see above , the lables is run=coffee-app which is a key value pair - key is run value is coffee-app. When we run Pod imperatively , kubectl ass this label to Pod.\nAdd custom label to Pod We can add label to Pod using kubectl label command.\nk8s@k8s-master-01:~$ kubectl label pod coffee-app app=frontend pod/coffee-app labeled k8s@k8s-master-01:~$  Here we have add a label app=frontend to pod coffee-app.\nUse label selectors Lets start another coffee application pod with name coffee-app02.\nk8s@k8s-master-01:~$ kubectl run coffee-app02 --image=ansilh/demo-coffee --restart=Never pod/coffee-app02 created k8s@k8s-master-01:~$  Now we have two Pods.\nk8s@k8s-master-01:~$ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS coffee-app 1/1 Running 0 5m5s app=frontend,run=coffee-app coffee-app02 1/1 Running 0 20s run=coffee-app02 k8s@k8s-master-01:~$  Lets see how can I select the Pods with label app=frontend.\nk8s@k8s-master-01:~$ kubectl get pods --selector=app=frontend NAME READY STATUS RESTARTS AGE coffee-app 1/1 Running 0 6m52s k8s@k8s-master-01:~$  You can add as many as label you want.\nWe can add a prefix like app ( eg: app/dev=true ) which is also a valid label.\n   Limitations      Prefix DNS subdomain with 256 characters   Key 63 characters   Value 63 characters    Remove labels See the labels of coffee-app\nk8s@k8s-master-01:~$ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS coffee-app 1/1 Running 0 28m app=frontend,run=coffee-app coffee-app02 1/1 Running 0 24m run=coffee-app02  Remove the app label\nk8s@k8s-master-01:~$ kubectl label pod coffee-app app- pod/coffee-app labeled  Resulting output\nk8s@k8s-master-01:~$ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS coffee-app 1/1 Running 0 29m run=coffee-app coffee-app02 1/1 Running 0 24m run=coffee-app02 k8s@k8s-master-01:~$  "
},
{
	"uri": "/04-labels_and_annotations/02-annotations/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Why we need annotations ? We can use either labels or annotations to attach metadata to Kubernetes objects. Labels can be used to select objects and to find collections of objects that satisfy certain conditions. In contrast, annotations are not used to identify and select objects. The metadata in an annotation can be small or large, structured or unstructured, and can include characters not permitted by labels.\nIts just a place to store more metadata which is not used for any selection , grouping or operations.\nAnnotate Pod Lets say , if you want to add a download URL to pod.\n$ kubectl annotate pod coffee-app url=https://hub.docker.com/r/ansilh/demo-webapp pod/coffee-app annotated  View annotations k8s@k8s-master-01:~$ kubectl describe pod coffee-app Name: coffee-app Namespace: default Priority: 0 PriorityClassName: \u0026lt;none\u0026gt; Node: k8s-worker-01/192.168.56.202 Start Time: Fri, 04 Jan 2019 00:47:10 +0530 Labels: app=frontend run=coffee-app Annotations: cni.projectcalico.org/podIP: 10.10.1.11/32 url: https://hub.docker.com/r/ansilh/demo-webapp Status: Running IP: 10.10.1.11 ...  Annotations filed containe two entries\ncni.projectcalico.org/podIP: 10.10.1.11/32\nurl: https://hub.docker.com/r/ansilh/demo-webapp\nRemove annotation Use same annotate command and mention only key with a dash (-) at the end of the key . Below command will remove the annotation url: https://hub.docker.com/r/ansilh/demo-webapp from Pod.\nk8s@k8s-master-01:~$ kubectl annotate pod coffee-app url- pod/coffee-app annotated k8s@k8s-master-01:~$  Annotation after removal.\nk8s@k8s-master-01:~$ kubectl describe pod coffee-app Name: coffee-app Namespace: default Priority: 0 PriorityClassName: \u0026lt;none\u0026gt; Node: k8s-worker-01/192.168.56.202 Start Time: Fri, 04 Jan 2019 00:47:10 +0530 Labels: app=frontend run=coffee-app Annotations: cni.projectcalico.org/podIP: 10.10.1.11/32 Status: Running IP: 10.10.1.11  "
},
{
	"uri": "/06-services/01-expose-pod/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Expose service running on Pod. Service A Coffee Pod running in cluster and its listening on port 9090 on Pod\u0026rsquo;s IP. How can we expose that service to external world so that users can access it ?\nWe need to expose the service.\nAs we know , the Pod IP is not routable outside of the cluster. So we need a mechanism to reach the host\u0026rsquo;s port and then that traffic should be diverted to Pod\u0026rsquo;s port.\nLets create a Pod Yaml first.\n$ vi coffe.yaml  apiVersion: v1 kind: Pod metadata: name: coffee spec: containers: - image: ansilh/demo-coffee name: coffee  Create Yaml\n$ kubectl create -f coffe.yaml  Expose the Pod with below command\n$ kubectl expose pod coffee --port=80 --target-port=9090 --type=NodePort  This will create a Service object in kubernetes , which will map the Node\u0026rsquo;s port 30836 to Service IP/Port 192.168.10.86:80\nWe can see the derails using kubectl get service command\n$ kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE coffee NodePort 192.168.10.86 \u0026lt;none\u0026gt; 80:30391/TCP 6s kubernetes ClusterIP 192.168.10.1 \u0026lt;none\u0026gt; 443/TCP 26h  We can also see that the port is listening and kube-proxy is the one listening on that port.\n$ sudo netstat -tnlup |grep 30836 tcp6 0 0 :::30391 :::* LISTEN 2785/kube-proxy  Now you can open browser and access the Coffee app using URL http://192.168.56.201:30391\nPorts in Service Objects nodePort This setting makes the service visible outside the Kubernetes cluster by the node’s IP address and the port number declared in this property. The service also has to be of type NodePort (if this field isn’t specified, Kubernetes will allocate a node port automatically).\nport Expose the service on the specified port internally within the cluster. That is, the service becomes visible on this port, and will send requests made to this port to the pods selected by the service.\ntargetPort This is the port on the pod that the request gets sent to. Your application needs to be listening for network requests on this port for the service to work.\n"
},
{
	"uri": "/07-multi_container_pod/01-env-inject/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Inject data to pod via Environmental variable We will create a Coffee Pod\n$ kubectl run tea --image=ansilh/demo-tea --env=MY_NODE_NAME=scratch --restart=Never --dry-run -o yaml \u0026gt;pod-with-env.yaml  apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: tea name: tea spec: containers: - env: - name: MY_NODE_NAME value: scratch image: ansilh/demo-tea name: coffee-new resources: {} dnsPolicy: ClusterFirst restartPolicy: Never status: {}  Lets run this Pod\n$ kubectl create -f pod-with-env.yaml  $ kubectl get pods NAME READY STATUS RESTARTS AGE tea 1/1 Running 0 7s  Lets expose the pod as NodePort\n$ kubectl expose pod tea --port=80 --target-port=8080 --type=NodePort  $ kubectl get svc tea NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE tea NodePort 192.168.10.37 \u0026lt;none\u0026gt; 80:32258/TCP 42s  Access the service using browser uisng node IP and port 32258\nYou will see below in Page Node:scratch\nExpose Pod fields to containers Lets extract the nodeName from spec ( Excuse me ? yeah we will see that in a moment )\nk8s@k8s-master-01:~$ kubectl get pods tea -o=jsonpath='{.spec.nodeName}' \u0026amp;\u0026amp; echo k8s-worker-01 k8s@k8s-master-01:~$ kubectl get pods tea -o=jsonpath='{.status.hostIP}' \u0026amp;\u0026amp; echo 192.168.56.202 k8s@k8s-master-01:~$ kubectl get pods tea -o=jsonpath='{.status.podIP}' \u0026amp;\u0026amp; echo 10.10.1.23 k8s@k8s-master-01:~$  To get the JSON path , first we need to get the entire object output in JSON. We have used output in YAML so far because its easy . But internally kubectl convers YAML to JSON\n$ kubectl get pod tea -o json  { \u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;, \u0026quot;kind\u0026quot;: \u0026quot;Pod\u0026quot;, \u0026quot;metadata\u0026quot;: { \u0026quot;annotations\u0026quot;: { \u0026quot;cni.projectcalico.org/podIP\u0026quot;: \u0026quot;10.10.1.23/32\u0026quot; }, \u0026quot;creationTimestamp\u0026quot;: \u0026quot;2019-01-06T15:09:36Z\u0026quot;, \u0026quot;labels\u0026quot;: { \u0026quot;run\u0026quot;: \u0026quot;tea\u0026quot; }, \u0026quot;name\u0026quot;: \u0026quot;tea\u0026quot;, \u0026quot;namespace\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;resourceVersion\u0026quot;: \u0026quot;218696\u0026quot;, \u0026quot;selfLink\u0026quot;: \u0026quot;/api/v1/namespaces/default/pods/tea\u0026quot;, \u0026quot;uid\u0026quot;: \u0026quot;14c1715b-11c5-11e9-9f0f-0800276a1bd2\u0026quot; }, \u0026quot;spec\u0026quot;: { \u0026quot;containers\u0026quot;: [ { \u0026quot;env\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;MY_NODE_NAME\u0026quot;, \u0026quot;value\u0026quot;: \u0026quot;scratch\u0026quot; } ], \u0026quot;image\u0026quot;: \u0026quot;ansilh/demo-tea\u0026quot;, \u0026quot;imagePullPolicy\u0026quot;: \u0026quot;Always\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;coffee-new\u0026quot;, \u0026quot;resources\u0026quot;: {}, \u0026quot;terminationMessagePath\u0026quot;: \u0026quot;/dev/termination-log\u0026quot;, \u0026quot;terminationMessagePolicy\u0026quot;: \u0026quot;File\u0026quot;, \u0026quot;volumeMounts\u0026quot;: [ { \u0026quot;mountPath\u0026quot;: \u0026quot;/var/run/secrets/kubernetes.io/serviceaccount\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;default-token-72pzg\u0026quot;, \u0026quot;readOnly\u0026quot;: true } ] } ], \u0026quot;dnsPolicy\u0026quot;: \u0026quot;ClusterFirst\u0026quot;, \u0026quot;enableServiceLinks\u0026quot;: true, \u0026quot;nodeName\u0026quot;: \u0026quot;k8s-worker-01\u0026quot;, \u0026quot;priority\u0026quot;: 0, \u0026quot;restartPolicy\u0026quot;: \u0026quot;Never\u0026quot;, \u0026quot;schedulerName\u0026quot;: \u0026quot;default-scheduler\u0026quot;, \u0026quot;securityContext\u0026quot;: {}, \u0026quot;serviceAccount\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;serviceAccountName\u0026quot;: \u0026quot;default\u0026quot;, \u0026quot;terminationGracePeriodSeconds\u0026quot;: 30, \u0026quot;tolerations\u0026quot;: [ { \u0026quot;effect\u0026quot;: \u0026quot;NoExecute\u0026quot;, \u0026quot;key\u0026quot;: \u0026quot;node.kubernetes.io/not-ready\u0026quot;, \u0026quot;operator\u0026quot;: \u0026quot;Exists\u0026quot;, \u0026quot;tolerationSeconds\u0026quot;: 300 }, { \u0026quot;effect\u0026quot;: \u0026quot;NoExecute\u0026quot;, \u0026quot;key\u0026quot;: \u0026quot;node.kubernetes.io/unreachable\u0026quot;, \u0026quot;operator\u0026quot;: \u0026quot;Exists\u0026quot;, \u0026quot;tolerationSeconds\u0026quot;: 300 } ], \u0026quot;volumes\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;default-token-72pzg\u0026quot;, \u0026quot;secret\u0026quot;: { \u0026quot;defaultMode\u0026quot;: 420, \u0026quot;secretName\u0026quot;: \u0026quot;default-token-72pzg\u0026quot; } } ] }, \u0026quot;status\u0026quot;: { \u0026quot;conditions\u0026quot;: [ { \u0026quot;lastProbeTime\u0026quot;: null, \u0026quot;lastTransitionTime\u0026quot;: \u0026quot;2019-01-06T15:09:36Z\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;True\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Initialized\u0026quot; }, { \u0026quot;lastProbeTime\u0026quot;: null, \u0026quot;lastTransitionTime\u0026quot;: \u0026quot;2019-01-06T15:09:42Z\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;True\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;Ready\u0026quot; }, { \u0026quot;lastProbeTime\u0026quot;: null, \u0026quot;lastTransitionTime\u0026quot;: \u0026quot;2019-01-06T15:09:42Z\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;True\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;ContainersReady\u0026quot; }, { \u0026quot;lastProbeTime\u0026quot;: null, \u0026quot;lastTransitionTime\u0026quot;: \u0026quot;2019-01-06T15:09:36Z\u0026quot;, \u0026quot;status\u0026quot;: \u0026quot;True\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;PodScheduled\u0026quot; } ], \u0026quot;containerStatuses\u0026quot;: [ { \u0026quot;containerID\u0026quot;: \u0026quot;docker://291a72e7fdab6a9f7afc47c640126cf596f5e071903b6a9055b44ef5bcb1c104\u0026quot;, \u0026quot;image\u0026quot;: \u0026quot;ansilh/demo-tea:latest\u0026quot;, \u0026quot;imageID\u0026quot;: \u0026quot;docker-pullable://ansilh/demo-tea@sha256:998d07a15151235132dae9781f587ea4d2822c62165778570145b0f659dda7bb\u0026quot;, \u0026quot;lastState\u0026quot;: {}, \u0026quot;name\u0026quot;: \u0026quot;coffee-new\u0026quot;, \u0026quot;ready\u0026quot;: true, \u0026quot;restartCount\u0026quot;: 0, \u0026quot;state\u0026quot;: { \u0026quot;running\u0026quot;: { \u0026quot;startedAt\u0026quot;: \u0026quot;2019-01-06T15:09:42Z\u0026quot; } } } ], \u0026quot;hostIP\u0026quot;: \u0026quot;192.168.56.202\u0026quot;, \u0026quot;phase\u0026quot;: \u0026quot;Running\u0026quot;, \u0026quot;podIP\u0026quot;: \u0026quot;10.10.1.23\u0026quot;, \u0026quot;qosClass\u0026quot;: \u0026quot;BestEffort\u0026quot;, \u0026quot;startTime\u0026quot;: \u0026quot;2019-01-06T15:09:36Z\u0026quot; } }  Remove below from pod-with-env.yaml\n- name: MY_NODE_NAME value: scratch  Add below Pod spec\n- name: MY_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName  Resulting Pod Yaml\napiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: tea name: tea spec: containers: - env: - name: MY_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName image: ansilh/demo-tea name: coffee-new resources: {} dnsPolicy: ClusterFirst restartPolicy: Never status: {}  Delete the running pod files\n$ kubectl delete pod tea  Create the pod with modified yaml file\n$ kubectl create -f pod-with-env.yaml  Make sure endpoint is up in service\n$ kubectl get ep tea NAME ENDPOINTS AGE tea 10.10.1.26:8080 31m  Refresh the browser page. This time you will see Node:k8s-worker-01\nLets do a cleanup on default namespace.\n$ kubectl delete --all pods $ kubectl delete --all services  Now you know - How to use export Objects in Yaml and Json format - How to access each fields using jsonpath - How to inject environmental variables to Pod - How to inject system generated fields to Pod using environmental variables\n"
},
{
	"uri": "/07-multi_container_pod/02-volumes/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Persistent volumes When a Pod dies , all container\u0026rsquo;s contents will be destroyed and never preserved by default. Sometimes you need to store the contents persistently (for eg:- etcd pod)\nKubernetes have a Volumes filed in Pod spec , which can be used to mount a volume inside container.\nLets explain the volume specs\n$ kubectl explain pod.spec.volumes  So when you write Yaml , you have to put volumes object in spec. As we have seen , volumes type is \u0026lt;[]Object\u0026gt; ; means its an array\nSo the contents below volumes should start with a dash \u0026ldquo;-\u0026rdquo;. Name is a mandatory field , so lets write those.\nspec: volumes: - name: \u0026quot;data\u0026quot;  We will use hostPath for now\n$ kubectl explain pod.spec.volumes.hostPath KIND: Pod VERSION: v1 RESOURCE: hostPath \u0026lt;Object\u0026gt; DESCRIPTION: HostPath represents a pre-existing file or directory on the host machine that is directly exposed to the container. This is generally used for system agents or other privileged things that are allowed to see the host machine. Most containers will NOT need this. More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath Represents a host path mapped into a pod. Host path volumes do not support ownership management or SELinux relabeling. FIELDS: path \u0026lt;string\u0026gt; -required- Path of the directory on the host. If the path is a symlink, it will follow the link to the real path. More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath type \u0026lt;string\u0026gt; Type for HostPath Volume Defaults to \u0026quot;\u0026quot; More info: https://kubernetes.io/docs/concepts/storage/volumes#hostpath k8s@k8s-master-01:~$  Host path needs a path on the host , so lets add that as well to the spec\nspec: volumes: - name: \u0026quot;data\u0026quot; hostPath: path: \u0026quot;/var/data\u0026quot;  This will add a volume to Pod\nNow we have to tell the pods to use it.\nIn containers specification, we have volumeMounts field which can be used to mount the volume.\n$ kubectl explain pod.spec.containers.volumeMounts KIND: Pod VERSION: v1 RESOURCE: volumeMounts \u0026lt;[]Object\u0026gt; DESCRIPTION: Pod volumes to mount into the container's filesystem. Cannot be updated. VolumeMount describes a mounting of a Volume within a container. FIELDS: mountPath \u0026lt;string\u0026gt; -required- Path within the container at which the volume should be mounted. Must not contain ':'. mountPropagation \u0026lt;string\u0026gt; mountPropagation determines how mounts are propagated from the host to container and the other way around. When not set, MountPropagationNone is used. This field is beta in 1.10. name \u0026lt;string\u0026gt; -required- This must match the Name of a Volume. readOnly \u0026lt;boolean\u0026gt; Mounted read-only if true, read-write otherwise (false or unspecified). Defaults to false. subPath \u0026lt;string\u0026gt; Path within the volume from which the container's volume should be mounted. Defaults to \u0026quot;\u0026quot; (volume's root).  volumeMounts is \u0026lt;[]Object\u0026gt; . mountPath is required and name\nname must match the Name of a Volume\nResulting Pod spec will become ;\nspec: volumes: - name: \u0026quot;data\u0026quot; hostPath: path: \u0026quot;/var/data\u0026quot; containers: - name: nginx image: nginx volumeMounts: - name: \u0026quot;data\u0026quot; mountPath: \u0026quot;/usr/share/nginx/html\u0026quot;  Lets add the basic fields to complete the Yaml and save the file as nginx.yaml\napiVersion: v1 kind: Pod metadata: name: nginx-pod01 spec: volumes: - name: \u0026quot;data\u0026quot; hostPath: path: \u0026quot;/var/data\u0026quot; containers: - name: nginx image: nginx volumeMounts: - name: \u0026quot;data\u0026quot; mountPath: \u0026quot;/usr/share/nginx/html\u0026quot;  Create the Pod\nkubectl create -f nginx.yaml  Check where its running.\n$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-pod01 1/1 Running 0 55s 10.10.1.27 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  Lets expose this Pod first.\n$ kubectl expose pod nginx-pod01 --port=80 --target-port=80 --type=NodePort  error: couldn't retrieve selectors via --selector flag or introspection: the pod has no labels and cannot be exposed See 'kubectl expose -h' for help and examples.  This indicates that we didn\u0026rsquo;t add label , because the service needs a label to map the Pod to endpoint\nLets add a label to the Pod.\n$ kubectl label pod nginx-pod01 run=nginx-pod01  Now we can we can expose the Pod\n$ kubectl expose pod nginx-pod01 --port=80 --target-port=80 --type=NodePort  Get the node port which service is listening to\n$ kubectl get svc nginx-pod01 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx-pod01 NodePort 192.168.10.51 \u0026lt;none\u0026gt; 80:31538/TCP 26s  You will get 403 Forbidden page , because there is no html page to load.\nNow we can go to the node where the Pod is running and check the path /var/data\nk8s@k8s-worker-01:~$ ls -ld /var/data drwxr-xr-x 2 root root 4096 Jan 7 00:52 /var/data k8s@k8s-worker-01:~$ cd /var/data k8s@k8s-worker-01:/var/data$ ls -lrt total 0 k8s@k8s-worker-01:/var/data$  Nothing is there.The directory is owned by root , so you have to create the file index.html with root.\nk8s@k8s-worker-01:/var/data$ sudo -i [sudo] password for k8s: root@k8s-worker-01:~# cd /var/data root@k8s-worker-01:/var/data# root@k8s-worker-01:/var/data# echo \u0026quot;This is a test page\u0026quot; \u0026gt;index.html root@k8s-worker-01:/var/data#  Reload the web page and you should see \u0026ldquo;This is a test page\u0026rdquo;\nNow you know; - How to create a volume. - How to mount a volume. - How to access the contents of volume from host.\n"
},
{
	"uri": "/07-multi_container_pod/03-init-container/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " InitContainer In this session , we will discuss about InitContainer\nNon-persistent web server As we already know ,containers are ephemeral and the modifications will be lost when container is destroyed.\nIn this example , we will download webpages from Github repository and store it in a emptyDir volume.\nFrom this emptyDir volume , we will serve the HTML pages using an Nginx Pod\nemptyDir is a volume type , just like hostPath , but the contents of emptyDir will be destroyed when Pod is stopped.\nSo lets write a Pod specification for Nginx container and add InitContainer to download HTML page\napiVersion: v1 kind: Pod metadata: labels: run: demo-web name: demo-web spec: volumes: - name: html emptyDir: {} containers: - image: nginx name: demo-web volumeMounts: - name: html mountPath: /usr/share/nginx/html initContainers: - image: ansilh/debug-tools name: git-pull args: - git - clone - https://github.com/ansilh/k8s-demo-web.git - /html/. volumeMounts: - name: html mountPath: /html/  Problem with this design is , no way to pull the changes once Pod is up. InitContainer run only once during the startup of the Pod.\nIncase of InitContainer failure , Pod startup will fail and never start other containers.\nWe can specify more than one initcontainer if needed. Startup of initcontainer will be sequential and the order will be selected based on the order in yaml spec.\nIn next session , we will discuss about other design patterns for Pod.\n"
},
{
	"uri": "/07-multi_container_pod/04-pod-patterns/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Pod design patterns When the containers have the exact same lifecycle, or when the containers must run on the same node. The most common scenario is that you have a helper process that needs to be located and managed on the same node as the primary container.\nAnother reason to combine containers into a single pod is for simpler communication between containers in the pod. These containers can communicate through shared volumes (writing to a shared file or directory) and through inter-process communication (semaphores or shared memory).\nThere are three common design patterns and use-cases for combining multiple containers into a single pod. We’ll walk through the sidecar pattern, the adapter pattern, and the ambassador pattern.\nExample #1: Sidecar containers Sidecar containers extend and enhance the “main” container, they take existing containers and make them better. As an example, consider a container that runs the Nginx web server. Add a different container that syncs the file system with a git repository, share the file system between the containers and you have built Git push-to-deploy.\napiVersion: v1 kind: Pod metadata: labels: run: demo-web name: demo-web spec: volumes: - name: html emptyDir: {} containers: - image: nginx name: demo-web volumeMounts: - name: html mountPath: /usr/share/nginx/html - image: ansilh/debug-tools name: git-pull args: - sh - -c - 'while true ; do [ ! -d /html/.git ] \u0026amp;\u0026amp; git clone https://github.com/ansilh/k8s-demo-web.git /html/ || { cd /html; git pull; } ; date; sleep 5 ; done' volumeMounts: - name: html mountPath: /html/  Lets do a tail on the logs and see how the git-pull works\n$ kubectl logs demo-web git-pull -f Cloning into '/html'... Fri Jan 11 20:39:25 UTC 2019 Already up to date. Fri Jan 11 20:39:31 UTC 2019  Lets modify the WebPage and push the changes to Github\nAlready up to date. Fri Jan 11 20:44:04 UTC 2019 From https://github.com/ansilh/k8s-demo-web e2df24f..1791ee1 master -\u0026gt; origin/master Updating e2df24f..1791ee1 Fast-forward images/pic-k8s.jpg | Bin 0 -\u0026gt; 14645 bytes index.html | 4 ++-- 2 files changed, 2 insertions(+), 2 deletions(-) create mode 100644 images/pic-k8s.jpg Fri Jan 11 20:44:10 UTC 2019 Already up to date.  Example #2: Ambassador containers Ambassador containers proxy a local connection to the world. As an example, consider a Redis cluster with read-replicas and a single write master. You can create a Pod that groups your main application with a Redis ambassador container. The ambassador is a proxy is responsible for splitting reads and writes and sending them on to the appropriate servers. Because these two containers share a network namespace, they share an IP address and your application can open a connection on “localhost” and find the proxy without any service discovery. As far as your main application is concerned, it is simply connecting to a Redis server on localhost. This is powerful, not just because of separation of concerns and the fact that different teams can easily own the components, but also because in the development environment, you can simply skip the proxy and connect directly to a Redis server that is running on localhost.\nExample #3: Adapter containers Adapter containers standardize and normalize output. Consider the task of monitoring N different applications. Each application may be built with a different way of exporting monitoring data. (e.g. JMX, StatsD, application specific statistics) but every monitoring system expects a consistent and uniform data model for the monitoring data it collects. By using the adapter pattern of composite containers, you can transform the heterogeneous monitoring data from different systems into a single unified representation by creating Pods that groups the application containers with adapters that know how to do the transformation. Again because these Pods share namespaces and file systems, the coordination of these two containers is simple and straightforward.\n"
},
{
	"uri": "/07-multi_container_pod/05-manual-schedule/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Pod Scheduling Node Selector Suppose you have a Pod which needs to be running on a Pod which is having SSD in it.\nFirst we need to add a label to the node which is having SSD\n$ kubectl label node k8s-worker-01 disktype=ssd  Now we can write a Pod spec with nodeSelector\napiVersion: v1 kind: Pod metadata: name: nginx labels: env: test spec: containers: - name: nginx image: nginx nodeSelector: disktype: ssd  Scheduler will look at the node selector and select apropriate node to run the pod\nnodeName  Kube-scheduler will find a suitable pod by evaluating the constraints. Scheduler will modify the value of .spec.nodeName of Pod object . kubelet will observe the change via API server and will start the pod based on the specification.  This means , we can manually specify the nodeName in Pod spec and schedule it.\nYou can read more about nodeName in below URL https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodename\n"
},
{
	"uri": "/07-multi_container_pod/06-taints-and-tolerations/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Taints and Tolerations You add a taint to a node using kubectl taint. For example,\n$ kubectl taint nodes k8s-worker-02 key=value:NoSchedule  places a taint on node node1. The taint has key key, value value, and taint effect NoSchedule. This means that no pod will be able to schedule onto node1 unless it has a matching toleration.\nTo remove the taint added by the command above, you can run:\nkubectl taint nodes k8s-worker-02 key:NoSchedule-  You specify a toleration for a pod in the PodSpec. Both of the following tolerations “match” the taint created by the kubectl taint line above, and thus a pod with either toleration would be able to schedule onto node1:\ntolerations: - key: \u0026quot;key\u0026quot; operator: \u0026quot;Equal\u0026quot; value: \u0026quot;value\u0026quot; effect: \u0026quot;NoSchedule\u0026quot;  tolerations: - key: \u0026quot;key\u0026quot; operator: \u0026quot;Exists\u0026quot; effect: \u0026quot;NoSchedule\u0026quot;  A toleration “matches” a taint if the keys are the same and the effects are the same, and:\n the operator is Exists (in which case no value should be specified), or the operator is Equal and the values are equal Operator defaults to Equal if not specified.  The above example used effect of NoSchedule. Alternatively, you can use effect of PreferNoSchedule. This is a “preference” or “soft” version of NoSchedule – the system will try to avoid placing a pod that does not tolerate the taint on the node, but it is not required. The third kind of effect is NoExecute\nNormally, if a taint with effect NoExecute is added to a node, then any pods that do not tolerate the taint will be evicted immediately, and any pods that do tolerate the taint will never be evicted. However, a toleration with NoExecute effect can specify an optional tolerationSeconds field that dictates how long the pod will stay bound to the node after the taint is added. For example,\ntolerations: - key: \u0026quot;key1\u0026quot; operator: \u0026quot;Equal\u0026quot; value: \u0026quot;value1\u0026quot; effect: \u0026quot;NoExecute\u0026quot; tolerationSeconds: 3600  "
},
{
	"uri": "/08-api_reference/01-api-access/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Kubernetes API Lets explore how API defines and organizes objects\nAPI organization API is organized to two groups , one is Core group and second on is Named Groups\nCore Group Contains all stable and core API objects\n/api (APIVersions) This endpoint will return core API version \u0026amp; API address itself\nExecute below commands if you are using Vagrant based setup If you are using kubeadm based setup , then skip this.\n$ sudo mkdir -p /etc/kubernetes/pki/ $ sudo cp /home/vagrant/PKI/ca.pem /etc/kubernetes/pki/ca.crt $ sudo cp /home/vagrant/PKI/k8s-master-01.pem /etc/kubernetes/pki/apiserver-kubelet-client.crt $ sudo cp /home/vagrant/PKI/k8s-master-01-key.pem /etc/kubernetes/pki/apiserver-kubelet-client.key  $ sudo curl -s --cacert /etc/kubernetes/pki/ca.crt --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key -XGET 'https://192.168.56.201:6443/api?timeout=32s' |python3 -m json.tool  { \u0026quot;kind\u0026quot;: \u0026quot;APIVersions\u0026quot;, \u0026quot;versions\u0026quot;: [ \u0026quot;v1\u0026quot; ], \u0026quot;serverAddressByClientCIDRs\u0026quot;: [ { \u0026quot;clientCIDR\u0026quot;: \u0026quot;0.0.0.0/0\u0026quot;, \u0026quot;serverAddress\u0026quot;: \u0026quot;192.168.56.201:6443\u0026quot; } ] }  /api/v1 (APIResourceList) This endpoint will return objects/resources in core group v1\n$ sudo curl -s --cacert /etc/kubernetes/pki/ca.crt --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key -XGET 'https://192.168.56.201:6443/api/v1?timeout=32s' |python3 -m json.tool  { \u0026quot;kind\u0026quot;: \u0026quot;APIResourceList\u0026quot;, \u0026quot;groupVersion\u0026quot;: \u0026quot;v1\u0026quot;, \u0026quot;resources\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;bindings\u0026quot;, \u0026quot;singularName\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;namespaced\u0026quot;: true, \u0026quot;kind\u0026quot;: \u0026quot;Binding\u0026quot;, \u0026quot;verbs\u0026quot;: [ \u0026quot;create\u0026quot; ] }, { \u0026quot;name\u0026quot;: \u0026quot;componentstatuses\u0026quot;, \u0026quot;singularName\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;namespaced\u0026quot;: false, \u0026quot;kind\u0026quot;: \u0026quot;ComponentStatus\u0026quot;, \u0026quot;verbs\u0026quot;: [ \u0026quot;get\u0026quot;, \u0026quot;list\u0026quot; ], \u0026quot;shortNames\u0026quot;: [ \u0026quot;cs\u0026quot; ] }, { \u0026quot;name\u0026quot;: \u0026quot;configmaps\u0026quot;, \u0026quot;singularName\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;namespaced\u0026quot;: true, \u0026quot;kind\u0026quot;: \u0026quot;ConfigMap\u0026quot;, \u0026quot;verbs\u0026quot;: [ \u0026quot;create\u0026quot;, \u0026quot;delete\u0026quot;, \u0026quot;deletecollection\u0026quot;, \u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;patch\u0026quot;, \u0026quot;update\u0026quot;, \u0026quot;watch\u0026quot; ], \u0026quot;shortNames\u0026quot;: [ \u0026quot;cm\u0026quot; ] }, ... ... ... { \u0026quot;name\u0026quot;: \u0026quot;services/status\u0026quot;, \u0026quot;singularName\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;namespaced\u0026quot;: true, \u0026quot;kind\u0026quot;: \u0026quot;Service\u0026quot;, \u0026quot;verbs\u0026quot;: [ \u0026quot;get\u0026quot;, \u0026quot;patch\u0026quot;, \u0026quot;update\u0026quot; ] } ] }  Named Groups /apis (APIGroupList) This endpoint will return all named groups\n$ sudo curl -s --cacert /etc/kubernetes/pki/ca.crt --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key -XGET 'https://192.168.56.201:6443/apis?timeout=32s' |python3 -m json.tool  /api/apps (APIGroup) $ sudo curl -s --cacert /etc/kubernetes/pki/ca.crt --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key -XGET 'https://192.168.56.201:6443/apis/apps?timeout=32s' |python3 -m json.tool  { \u0026quot;kind\u0026quot;: \u0026quot;APIGroup\u0026quot;, \u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;, \u0026quot;name\u0026quot;: \u0026quot;apps\u0026quot;, \u0026quot;versions\u0026quot;: [ { \u0026quot;groupVersion\u0026quot;: \u0026quot;apps/v1\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;v1\u0026quot; }, { \u0026quot;groupVersion\u0026quot;: \u0026quot;apps/v1beta2\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;v1beta2\u0026quot; }, { \u0026quot;groupVersion\u0026quot;: \u0026quot;apps/v1beta1\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;v1beta1\u0026quot; } ], \u0026quot;preferredVersion\u0026quot;: { \u0026quot;groupVersion\u0026quot;: \u0026quot;apps/v1\u0026quot;, \u0026quot;version\u0026quot;: \u0026quot;v1\u0026quot; } }  /api/apps/v1 (APIResourceList) Will return objects / resources under apps/v1\n$ sudo curl -s --cacert /etc/kubernetes/pki/ca.crt --cert /etc/kubernetes/pki/apiserver-kubelet-client.crt --key /etc/kubernetes/pki/apiserver-kubelet-client.key -XGET 'https://192.168.56.201:6443/apis/apps/v1?timeout=32s' |python3 -m json.tool  { \u0026quot;kind\u0026quot;: \u0026quot;APIResourceList\u0026quot;, \u0026quot;apiVersion\u0026quot;: \u0026quot;v1\u0026quot;, \u0026quot;groupVersion\u0026quot;: \u0026quot;apps/v1\u0026quot;, \u0026quot;resources\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;controllerrevisions\u0026quot;, \u0026quot;singularName\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;namespaced\u0026quot;: true, \u0026quot;kind\u0026quot;: \u0026quot;ControllerRevision\u0026quot;, \u0026quot;verbs\u0026quot;: [ \u0026quot;create\u0026quot;, \u0026quot;delete\u0026quot;, \u0026quot;deletecollection\u0026quot;, \u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;patch\u0026quot;, \u0026quot;update\u0026quot;, \u0026quot;watch\u0026quot; ] }, { \u0026quot;name\u0026quot;: \u0026quot;daemonsets\u0026quot;, \u0026quot;singularName\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;namespaced\u0026quot;: true, \u0026quot;kind\u0026quot;: \u0026quot;DaemonSet\u0026quot;, \u0026quot;verbs\u0026quot;: [ \u0026quot;create\u0026quot;, \u0026quot;delete\u0026quot;, \u0026quot;deletecollection\u0026quot;, \u0026quot;get\u0026quot;, \u0026quot;list\u0026quot;, \u0026quot;patch\u0026quot;, \u0026quot;update\u0026quot;, \u0026quot;watch\u0026quot; ], \u0026quot;shortNames\u0026quot;: [ \u0026quot;ds\u0026quot; ], \u0026quot;categories\u0026quot;: [ \u0026quot;all\u0026quot; ] }, ... ... ... { \u0026quot;name\u0026quot;: \u0026quot;statefulsets/status\u0026quot;, \u0026quot;singularName\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;namespaced\u0026quot;: true, \u0026quot;kind\u0026quot;: \u0026quot;StatefulSet\u0026quot;, \u0026quot;verbs\u0026quot;: [ \u0026quot;get\u0026quot;, \u0026quot;patch\u0026quot;, \u0026quot;update\u0026quot; ] } ] }  API versions Different API versions imply different levels of stability and support\nAlpha level:  The version names contain alpha (e.g. v1alpha1). May be buggy. Enabling the feature may expose bugs. Disabled by default. Support for feature may be dropped at any time without notice. The API may change in incompatible ways in a later software release without notice. Recommended for use only in short-lived testing clusters, due to increased risk of bugs and lack of long-term support.  Beta level:  The version names contain beta (e.g. v2beta3). Code is well tested. Enabling the feature is considered safe. Enabled by default. Support for the overall feature will not be dropped, though details may change. The schema and/or semantics of objects may change in incompatible ways in a subsequent beta or stable release. When this happens, k8s developers will provide instructions for migrating to the next version. This may require deleting, editing, and re-creating API objects. The editing process may require some thought. This may require downtime for applications that rely on the feature. Recommended for only non-business-critical uses because of potential for incompatible changes in subsequent releases. If you have multiple clusters which can be upgraded independently, you may be able to relax this restriction. Please do try our beta features and give feedback on them! Once they exit beta, it may not be practical for us to make more changes.  Stable level: The version name is vX where X is an integer. Stable versions of features will appear in released software for many subsequent versions\nList API version using kubectl API Versions $ kubectl api-versions admissionregistration.k8s.io/v1beta1 apiextensions.k8s.io/v1beta1 apiregistration.k8s.io/v1 apiregistration.k8s.io/v1beta1 apps/v1 apps/v1beta1 apps/v1beta2 authentication.k8s.io/v1 authentication.k8s.io/v1beta1 authorization.k8s.io/v1 authorization.k8s.io/v1beta1 autoscaling/v1 autoscaling/v2beta1 autoscaling/v2beta2 batch/v1 batch/v1beta1 certificates.k8s.io/v1beta1 coordination.k8s.io/v1beta1 crd.projectcalico.org/v1 events.k8s.io/v1beta1 extensions/v1beta1 networking.k8s.io/v1 policy/v1beta1 rbac.authorization.k8s.io/v1 rbac.authorization.k8s.io/v1beta1 scheduling.k8s.io/v1beta1 storage.k8s.io/v1 storage.k8s.io/v1beta1 v1  API resources $ kubectl api-resources NAME SHORTNAMES APIGROUP NAMESPACED KIND bindings true Binding componentstatuses cs false ComponentStatus configmaps cm true ConfigMap endpoints ep true Endpoints events ev true Event limitranges limits true LimitRange namespaces ns false Namespace nodes no false Node persistentvolumeclaims pvc true PersistentVolumeClaim persistentvolumes pv false PersistentVolume pods po true Pod podtemplates true PodTemplate replicationcontrollers rc true ReplicationController resourcequotas quota true ResourceQuota secrets true Secret serviceaccounts sa true ServiceAccount services svc true Service mutatingwebhookconfigurations admissionregistration.k8s.io false MutatingWebhookConfiguration validatingwebhookconfigurations admissionregistration.k8s.io false ValidatingWebhookConfiguration customresourcedefinitions crd,crds apiextensions.k8s.io false CustomResourceDefinition apiservices apiregistration.k8s.io false APIService controllerrevisions apps true ControllerRevision daemonsets ds apps true DaemonSet deployments deploy apps true Deployment replicasets rs apps true ReplicaSet statefulsets sts apps true StatefulSet tokenreviews authentication.k8s.io false TokenReview localsubjectaccessreviews authorization.k8s.io true LocalSubjectAccessReview selfsubjectaccessreviews authorization.k8s.io false SelfSubjectAccessReview selfsubjectrulesreviews authorization.k8s.io false SelfSubjectRulesReview subjectaccessreviews authorization.k8s.io false SubjectAccessReview horizontalpodautoscalers hpa autoscaling true HorizontalPodAutoscaler cronjobs cj batch true CronJob jobs batch true Job certificatesigningrequests csr certificates.k8s.io false CertificateSigningRequest leases coordination.k8s.io true Lease bgpconfigurations crd.projectcalico.org false BGPConfiguration bgppeers crd.projectcalico.org false BGPPeer clusterinformations crd.projectcalico.org false ClusterInformation felixconfigurations crd.projectcalico.org false FelixConfiguration globalnetworkpolicies crd.projectcalico.org false GlobalNetworkPolicy globalnetworksets crd.projectcalico.org false GlobalNetworkSet hostendpoints crd.projectcalico.org false HostEndpoint ippools crd.projectcalico.org false IPPool networkpolicies crd.projectcalico.org true NetworkPolicy events ev events.k8s.io true Event daemonsets ds extensions true DaemonSet deployments deploy extensions true Deployment ingresses ing extensions true Ingress networkpolicies netpol extensions true NetworkPolicy podsecuritypolicies psp extensions false PodSecurityPolicy replicasets rs extensions true ReplicaSet networkpolicies netpol networking.k8s.io true NetworkPolicy poddisruptionbudgets pdb policy true PodDisruptionBudget podsecuritypolicies psp policy false PodSecurityPolicy clusterrolebindings rbac.authorization.k8s.io false ClusterRoleBinding clusterroles rbac.authorization.k8s.io false ClusterRole rolebindings rbac.authorization.k8s.io true RoleBinding roles rbac.authorization.k8s.io true Role priorityclasses pc scheduling.k8s.io false PriorityClass storageclasses sc storage.k8s.io false StorageClass volumeattachments storage.k8s.io false VolumeAttachment  "
},
{
	"uri": "/09-configmaps_and_secrets/01-private-registry/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Private Registry using Harbor All container images that we used in past examples are downloaded from docker hub public registry. But in a production environment , we use private image registries so that we will have better control of application lifecycle.\nIn this session , we will deploy a private registry using Harbor\nStudents needs to deploy this in a separate VM (4GB memmory + 2vCPUs). If you are attending live session , then instructor will provide private registry credential.\nDownload docker-compose binary $ sudo curl -L \u0026quot;https://github.com/docker/compose/releases/download/1.23.2/docker-compose-$(uname -s)-$(uname -m)\u0026quot; -o /usr/local/bin/docker-compose $ sudo chmod +x /usr/local/bin/docker-compose $ sudo ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose  Clone Portus git repository $ sudo curl https://storage.googleapis.com/harbor-releases/release-1.7.0/harbor-offline-installer-v1.7.1.tgz -O  $ tar -xvf harbor-offline-installer-v1.7.1.tgz  Mandatory configuration Go to the extracted directory\n$ cd harbor  Edit configuration file harbor.cfg\n$ vi harbor.cfg  Change hostname to the server\u0026rsquo;s FQDN . Make sure you have proper name resolution in place or create an entry in /etc/hosts Change ui_url_protocol from http to https\nStart the environment with docker-compose $ docker-compose up -d  It will take some time to download and deploy all needed containers.\nOnce all services were up , access the registry using FQDN or IP\nDefault username \u0026amp; password is admin/Harbor12345 "
},
{
	"uri": "/10-deployments/01-deployment/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Create an Nginx Deployment $ kubectl run nginx --image=nginx  Output\ndeployment.apps/nginx created  Verify the Pods running\n$ kubectl get pods  Output\nNAME READY STATUS RESTARTS AGE nginx-7cdbd8cdc9-9xsms 1/1 Running 0 27s  Here we can see that the Pod name is not like the usual one.\nLets delete the Pod and see what will happen.\n$ kubectl delete pod nginx-7cdbd8cdc9-9xsms  Output\npod \u0026quot;nginx-7cdbd8cdc9-9xsms\u0026quot; deleted  Verify Pod status\n$ kubectl get pods  Output\nNAME READY STATUS RESTARTS AGE nginx-7cdbd8cdc9-vfbn8 1/1 Running 0 81s  A new Pod has been created again !!!\n"
},
{
	"uri": "/11-daemonsets/01-deamonset/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " DaemonSet A DaemonSet ensures that all (or some) Nodes run a copy of a Pod. As nodes are added to the cluster, Pods are added to them. As nodes are removed from the cluster, those Pods are garbage collected. Deleting a DaemonSet will clean up the Pods it created.\nLets imagine that we need an agent deployed on all nodes which reads the system logs and sent to a log analysis database\nHere we are mimicking the agent using a simple pod. A pod that mounts /var/log inside the pod and do tail of syslog file\n$ vi logger.yaml  apiVersion: v1 kind: Pod metadata: name: log-tailer spec: volumes: - name: syslog hostPath: path: /var/log containers: - name: logger image: ansilh/debug-tools args: - /bin/sh - -c - tail -f /data/logs/syslog volumeMounts: - name: syslog mountPath: /data/logs/ securityContext: privileged: true  $ kubectl create -f logger.yaml  Now we can execute a logs command to see the system log\n$ kubectl logs log-tailer -f  $ kubectl delete pod log-tailer  Now we need the same kind of Pod to be running on all nodes. If we add a node in future , the same pod should start on that node as well.\nTo accomplish this goal , we can use DaemonSet.\n$ vi logger.yaml  apiVersion: apps/v1 kind: DaemonSet metadata: name: log-tailer spec: selector: matchLabels: name: log-tailer template: metadata: labels: name: log-tailer spec: tolerations: - key: node-role.kubernetes.io/master effect: NoSchedule volumes: - name: syslog hostPath: path: /var/log containers: - name: logger image: ansilh/debug-tools args: - /bin/sh - -c - tail -f /data/logs/syslog volumeMounts: - name: syslog mountPath: /data/logs/ securityContext: privileged: true  $ kubectl create -f logger.yaml  $ kubectl get pods -o wide  NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES log-tailer-hzjzx 1/1 Running 0 22s 10.10.36.242 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; log-tailer-rqgrf 1/1 Running 0 22s 10.10.151.153 k8s-master-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  Important notes at the end of the page in this URL : https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/\n"
},
{
	"uri": "/15-k8s_from_sratch/01-api-access-controll/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " API Access Control Users access the API using kubectl, client libraries, or by making REST requests. Both human users and Kubernetes service accounts can be authorized for API access. When a request reaches the API, it goes through several stages, illustrated in the following diagram:\nAuthentication Once TLS is established, the HTTP request moves to the Authentication step. This is shown as step 1 in the diagram.\nWe use X509 Client Certs for authentication.\nWhen a client certificate is presented and verified, the common name (CN) of the subject is used as the user name for the request.\nClient certificates can also indicate a user’s group memberships using the certificate’s organization fields (O). To include multiple group memberships for a user, include multiple organization fields in the certificate.\nWhile Kubernetes uses usernames for access control decisions and in request logging, it does not have a user object nor does it store usernames or other information about users in its object store.\nAuthorization After the request is authenticated as coming from a specific user, the request must be authorized. This is shown as step 2 in the diagram.\nA request must include the username of the requester, the requested action, and the object affected by the action. The request is authorized if an existing role and role mapping declares that the user has permissions to complete the requested action.\nAdmission Control Admission Control Modules are software modules that can modify or reject requests. This is shown as step 3 in the diagram. In addition to rejecting objects, admission controllers can also set complex defaults for fields. Once a request passes all admission controllers, it is validated using the validation routines for the corresponding API object, and then written to the object store (shown as step 4).\nExample of an Admission Controller is here\n"
},
{
	"uri": "/01-introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": " Chapter 1 Introduction In this chapter we will discuss about the basic building blocks of containers with few demos and architecture of kubernetes\n"
},
{
	"uri": "/03-pods/02-create-pod/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Pod - Declarative Way After completing this session , you will be able to create Pod declaratively and will be able to login to check services running on other pods.\nSo lets get started.\nLets Check the running Pods k8s@k8s-master-01:~$ kubectl get pods No resources found. k8s@k8s-master-01:~$  Nothing \nLets create one using a YAML file $ vi pod.yaml  apiVersion: v1 kind: Pod metadata: name: coffee-app spec: containers: - image: ansilh/demo-coffee name: coffee  Apply YAML using kubectl command $ kubectl apply -f pod.yaml  View status of Pod Pod status is ContainerCreating\n$ kubectl get pods  Output\nNAME READY STATUS RESTARTS AGE coffee-app 0/1 ContainerCreating 0 4s  Execute kubectl get pods after some time Now Pod status will change to Running\n$ kubectl get pods  Output\nNAME READY STATUS RESTARTS AGE coffee-app 1/1 Running 0 27s  Now we can see our first Pod \nGet the IP address of Pod $ kubectl get pods -o wide  Output\nNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES coffee-app 1/1 Running 0 2m8s 192.168.1.7 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  Create a new CentOS container $ vi centos-pod.yaml  apiVersion: v1 kind: Pod metadata: name: centos-pod spec: containers: - image: tutum/centos name: centos  Apply the Yaml spec $ kubectl apply -f centos-pod.yaml  Verify the status of Pod $ kubectl get pods  NAME READY STATUS RESTARTS AGE centos-pod 0/1 ContainerCreating 0 12s coffee-app 1/1 Running 0 5m31s  After some time status will change to Running $ kubectl get pods  NAME READY STATUS RESTARTS AGE centos-pod 1/1 Running 0 59s coffee-app 1/1 Running 0 6m18s  Login to CentOS Pod $ kubectl exec -it centos-pod -- /bin/bash  Verify Coffee app using curl $ curl -s 192.168.1.13:9090 |grep 'Serving'  \u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;/head\u0026gt;\u0026lt;title\u0026gt;\u0026lt;/title\u0026gt;\u0026lt;body\u0026gt;\u0026lt;div\u0026gt; \u0026lt;h2\u0026gt;Serving Coffee from\u0026lt;/h2\u0026gt;\u0026lt;h3\u0026gt;Pod:coffee-app\u0026lt;/h3\u0026gt;\u0026lt;h3\u0026gt;IP:192.168.1.13\u0026lt;/h3\u0026gt;\u0026lt;h3\u0026gt;Node:172.16.0.1\u0026lt;/h3\u0026gt;\u0026lt;img src=\u0026quot;data:image/png;base64, [root@centos-pod /]#  Delete pod $ kubectl delete pod coffee-app centos-pod  pod \u0026quot;coffee-app\u0026quot; deleted pod \u0026quot;centos-pod\u0026quot; deleted  Make sure not pod is running $ kubectl get pods  "
},
{
	"uri": "/06-services/02-nodeport/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " NodePort NodePort Exposes the service on each Node’s IP at a static port (the NodePort). A ClusterIP service, to which the NodePort service will route, is automatically created. You’ll be able to contact the NodePort service, from outside the cluster, by requesting :.\nHow nodePort works kube-proxy watches the Kubernetes master for the addition and removal of Service and Endpoints objects.\n(We will discuss about Endpoints later in this session.)\nFor each Service, it opens a port (randomly chosen) on the local node. Any connections to this “proxy port” will be proxied to one of the Service’s backend Pods (as reported in Endpoints). Lastly, it installs iptables rules which capture traffic to the Service’s clusterIP (which is virtual) and Port and redirects that traffic to the proxy port which proxies the backend Pod.\nnodePort workflow.  nodePort -\u0026gt; 30391\n port -\u0026gt; 80\n targetPort -\u0026gt; 9090\n  "
},
{
	"uri": "/08-api_reference/02-swagger-ui/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Swagger API explorer Enable swagger We can enable swagger UI in API Server\n Added \u0026ndash;enable-swagger-ui=true to API manifest file /etc/kubernetes/manifests/kube-apiserver.yaml (only applicable to kubeadm deployments ) Save the file API pod will restart itself Make sure API server pod is up\n$ kubectl get pods -n kube-system |grep kube-apiserver kube-apiserver-k8s-master-01 1/1 Running 0 55s   Enable API proxy access console $ kubectl proxy --port=8080   Open an SSH tunnel from local system to server port 8080\n Access API swagger UI using webbrowser using URL http://localhost:8080/swagger-ui/   Note: Swagger UI is very slow because of the design of Swagger itself. Kubernetes may drop Swagger UI from API server. Github Issue\nYou can read more about API here\n"
},
{
	"uri": "/09-configmaps_and_secrets/02-configmaps/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " ConfigMaps In this session , we will explore the use of ConfigMaps.\nIf you want to customize the configuration of an application inside a Pod , you have to change the configuration files inside the container and then we have to wait for the application to re-read the updated configuration file.\nWhen Pod lifecycle ends , the changes we made will be lost and we have to redo the same changes when the Pod comes-up.\nThis is not convenient and we need a better way to manage these configuration related operations.\nTo achieve a persistent configuration regardless of the Pod state , k8s introduced ConfigMaps.\nWe can store environmental variables or a file content or both using ConfigMaps in k8s.\nUse the kubectl create configmap command to create configmaps from directories, files, or literal values:\nwhere  is the name you want to assign to the ConfigMap and  is the directory, file, or literal value to draw the data from.\nThe data source corresponds to a key-value pair in the ConfigMap, where\nkey = the file name or the key you provided on the command line, and value = the file contents or the literal value you provided on the command line. You can use kubectl describe or kubectl get to retrieve information about a ConfigMap\nCreate ConfigMap from literals - Declarative apiVersion: v1 kind: ConfigMap metadata: name: myconfig data: VAR1: val1  Create ConfigMap from literals - Imperative $ kubectl create configmap myconfig --from-literal=VAR1=val1  Create ConfigMap from file - Declarative apiVersion: v1 kind: ConfigMap metadata: name: myconfig data: configFile: | This content is coming from a file Also this file have multiple lines  Create ConfigMap from file - Imperative $ cat \u0026lt;\u0026lt;EOF \u0026gt;configFile This content is coming from a file EOF  $ cat configFile  $ kubectl create configmap myconfig --from-file=configFile  "
},
{
	"uri": "/10-deployments/02-expose-deployment/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Expose Nginx Deployment We know how to expose a Pod using a service.\nThe endpoints will be created based on the label of the Pod.\nHere how we can create a service which can be used to access Nginx from outside\nFirst we will check the label of the Pod\n$ kubectl get pod nginx-7cdbd8cdc9-vfbn8 --show-labels  Output\nNAME READY STATUS RESTARTS AGE LABELS nginx-7cdbd8cdc9-vfbn8 1/1 Running 0 7m19s pod-template-hash=7cdbd8cdc9,run=nginx  As you can see , one of the label is run=nginx\nNext write a Service spec and use selector as run: nginx\n$ vi nginx-svc.yaml  apiVersion: v1 kind: Service metadata: creationTimestamp: null labels: run: nginx-svc name: nginx-svc spec: ports: - port: 80 protocol: TCP targetPort: 80 selector: run: nginx type: LoadBalancer  This service will look for Pods with label \u0026ldquo;run=nginx\u0026rdquo;\n$ kubectl apply -f nginx-svc.yaml  Verify the service details\n$ kubectl get svc  Output\nNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 172.168.0.1 \u0026lt;none\u0026gt; 443/TCP 103m nginx-svc LoadBalancer 172.168.47.182 192.168.31.201 80:32369/TCP 3s  Now we will be able to see the default nginx page with IP 192.168.31.201\n"
},
{
	"uri": "/15-k8s_from_sratch/02-certificates-client-tools/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Tools Installation 1. Install cfssl to generate certificates $ wget -q --show-progress --https-only --timestamping \\ https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 \\ https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64  $ chmod +x cfssl_linux-amd64 cfssljson_linux-amd64  $ sudo mv cfssl_linux-amd64 /usr/local/bin/cfssl $ sudo mv cfssljson_linux-amd64 /usr/local/bin/cfssljson   Verification  $ cfssl version   Output  Version: 1.2.0 Revision: dev Runtime: go1.6  2. Install kubectl  Download kubectl  wget https://storage.googleapis.com/kubernetes-release/release/v1.12.0/bin/linux/amd64/kubectl   Make it executable and move to one of the shell $PATH  $ chmod +x kubectl $ sudo mv kubectl /usr/local/bin/   Verification  $ kubectl version --client   Output  Client Version: version.Info{Major:\u0026quot;1\u0026quot;, Minor:\u0026quot;12\u0026quot;, GitVersion:\u0026quot;v1.12.0\u0026quot;, GitCommit:\u0026quot;0ed33881dc4355495f623c6f22e7dd0b7632b7c0\u0026quot;, GitTreeState:\u0026quot;clean\u0026quot;, BuildDate:\u0026quot;2018-09-27T17:05:32Z\u0026quot;, GoVersion:\u0026quot;go1.10.4\u0026quot;, Compiler:\u0026quot;gc\u0026quot;, Platform:\u0026quot;linux/amd64\u0026quot;}  "
},
{
	"uri": "/02-installation/",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": " Chapter 2 Installation In this chapter we will install virtualbox and setup networking. We will lern how to install and configure Docker Also we will install a two node kubernete cluster using kubeadm.\n"
},
{
	"uri": "/03-pods/03-create-pod/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Create Pod - Imperative Way Execute kubectl command to create a Pod. $ kubectl run coffee --image=ansilh/demo-coffee --restart=Never pod/coffee created  Verify Pod status $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES coffee 0/1 ContainerCreating 0 6s \u0026lt;none\u0026gt; k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES coffee 1/1 Running 0 19s 192.168.1.15 k8s-worker-01 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt;  Start a CentOS container $ kubectl run centos-pod --image=tutum/centos --restart=Never pod/centos-pod created  verify status of the Pod ; it should be in Running $ kubectl get pods NAME READY STATUS RESTARTS AGE centos-pod 1/1 Running 0 25s coffee 1/1 Running 0 2m10s  Logon to CentOS Pod $ kubectl exec -it centos-pod -- /bin/bash [root@centos-pod /]#  Verify Coffee App status [root@centos-pod /]# curl -s 192.168.1.15:9090 |grep 'Serving Coffee' \u0026lt;html\u0026gt;\u0026lt;head\u0026gt;\u0026lt;/head\u0026gt;\u0026lt;title\u0026gt;\u0026lt;/title\u0026gt;\u0026lt;body\u0026gt;\u0026lt;div\u0026gt; \u0026lt;h2\u0026gt;Serving Coffee from\u0026lt;/h2\u0026gt;\u0026lt;h3\u0026gt;Pod:coffee\u0026lt;/h3\u0026gt;\u0026lt;h3\u0026gt;IP:192.168.1.15\u0026lt;/h3\u0026gt;\u0026lt;h3\u0026gt;Node:172.16.0.1\u0026lt;/h3\u0026gt;\u0026lt;img src=\u0026quot;data:image/png;base64, [root@centos-pod /]# exit  Delete pod k8s@k8s-master-01:~$ kubectl delete pod coffee centos-pod pod \u0026quot;coffee\u0026quot; deleted pod \u0026quot;centos-pod\u0026quot; deleted k8s@k8s-master-01:~$ kubectl get pods No resources found. k8s@k8s-master-01:~$  "
},
{
	"uri": "/06-services/03-clusterip/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Service with type clusterIP It exposes the service on a cluster-internal IP.\nWhen we expose a pod using kubectl expose command , we are creating a service object in API.\nChoosing this value makes the service only reachable from within the cluster. This is the default ServiceType.\nWe can see the Service spec using --dry-run \u0026amp; --output=yaml\n$ kubectl expose pod coffee --port=80 --target-port=9090 --type=ClusterIP --dry-run --output=yaml  Output\napiVersion: v1 kind: Service metadata: creationTimestamp: null labels: run: coffee name: coffee spec: ports: - port: 80 protocol: TCP targetPort: 9090 selector: run: coffee type: ClusterIP status: loadBalancer: {}  Cluster IP service is useful when you don\u0026rsquo;t want to expose the service to external world. eg:- database service.\nWith service names , a frontend tier can access the database backend without knowing the IPs of the Pods.\nCoreDNS (kube-dns) will dynamically create a service DNS entry and that will be resolvable from Pods.\nVerify Service DNS Start debug-tools container which is an alpine linux image with network related binaries\n$ kubectl run debugger --image=ansilh/debug-tools --restart=Never  $ kubectl exec -it debugger -- /bin/sh / # nslookup coffee Server: 192.168.10.10 Address: 192.168.10.10#53 Name: coffee.default.svc.cluster.local Address: 192.168.10.86 / # nslookup 192.168.10.86 86.10.168.192.in-addr.arpa name = coffee.default.svc.cluster.local. / #  coffee.default.svc.cluster.local ^ ^ ^ k8s domain | | | |-----------| | | +--------------- Indicates that its a service | +---------------------- Namespace +----------------------------- Service Name  "
},
{
	"uri": "/09-configmaps_and_secrets/03-access-configmap/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Define container environment variables using ConfigMap data Define a container environment variable with data from a single ConfigMap  Define an environment variable as a key-value pair in a ConfigMap:  $ kubectl create configmap special-config --from-literal=special.how=very   Assign the special.how value defined in the ConfigMap to the SPECIAL_LEVEL_KEY environment variable in the Pod specification.  apiVersion: v1 kind: Pod metadata: name: test-pod spec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ \u0026quot;/bin/sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;env\u0026quot; ] env: # Define the environment variable - name: SPECIAL_LEVEL_KEY valueFrom: configMapKeyRef: # The ConfigMap containing the value you want to assign to SPECIAL_LEVEL_KEY name: special-config # Specify the key associated with the value key: special.how restartPolicy: Never  Configure all key-value pairs in a ConfigMap as container environment variables  Create a ConfigMap containing multiple key-value pairs.  apiVersion: v1 kind: ConfigMap metadata: name: special-config namespace: default data: SPECIAL_LEVEL: very SPECIAL_TYPE: charm   Use envFrom to define all of the ConfigMap’s data as container environment variables. The key from the ConfigMap becomes the environment variable name in the Pod.  apiVersion: v1 kind: Pod metadata: name: dapi-test-pod spec: containers: - name: test-container image: k8s.gcr.io/busybox command: [ \u0026quot;/bin/sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;env\u0026quot; ] envFrom: - configMapRef: name: special-config restartPolicy: Never  More about configmap can bre read from below link. https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/\n"
},
{
	"uri": "/09-configmaps_and_secrets/04-secrets/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Secrets A Secret is an object that contains a small amount of sensitive data\nTo use a secret, a pod needs to reference the secret. A secret can be used with a pod in two ways: as files in a volume mounted on one or more of its containers, or used by kubelet when pulling images for the pod\nSecrets will be stored as base64 encoded values and it will be used mostly during creation of an object\nCreating Secrets From variables $ kubectl create secret generic my-secret --from-literal=password=mypassword --dry-run -o yaml  From files $ kubectl create secret generic my-secret --from-file=user=user.txt --from-file=password.txt --dry-run -o yaml  $ echo root \u0026gt;user.txt $ echo password \u0026gt;password.txt  $ kubectl create secret generic my-secret --from-file=user=user.txt --from-file=password=password.txt --dry-run -o yaml  "
},
{
	"uri": "/09-configmaps_and_secrets/05-use-secrets/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Secrets Using secrets We can use secrets as environmental variable as well as mounts inside a Pod\nInjecting as environmental variable $ vi pod-secret.yaml  apiVersion: v1 kind: Pod metadata: labels: run: debugger name: debugger spec: containers: - image: ansilh/debug-tools name: debugger env: - name: USER valueFrom: secretKeyRef: name: my-secret key: user - name: PASSWORD valueFrom: secretKeyRef: name: my-secret key: password  $ kubectl create -f pod-secret.yaml  $ kubectl get pods NAME READY STATUS RESTARTS AGE debugger 1/1 Running 0 17s  Logon to container and verify the environmental variables\n$ kubectl exec -it debugger -- /bin/sh  Verify environment variables inside Pod\n/ # echo $USER root / # echo $PASSWORD mypassword / #  Delete the Pod\n$ kubectl delete pod debugger  Mounting as files using volumes $ vi pod-secret.yaml  apiVersion: v1 kind: Pod metadata: labels: run: debugger name: debugger spec: volumes: - name: secret secret: secretName: my-secret containers: - image: ansilh/debug-tools name: debugger volumeMounts: - name: secret mountPath: /data  $ kubectl create -f pod-secret.yaml  $ kubectl exec -it debugger -- /bin/sh  / # cd /data /data # /data # cat user root /data # cat password mypassword /data #  "
},
{
	"uri": "/10-deployments/03-scaling/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Scale Deployments When load increases , we can scale the pods using deployment scaling\n$ kubectl scale deployment --replicas=3 nginx  $ kubectl get pods  Output\nNAME READY STATUS RESTARTS AGE nginx-7cdbd8cdc9-4lhh4 1/1 Running 0 6s nginx-7cdbd8cdc9-mxhnl 1/1 Running 0 6s nginx-7cdbd8cdc9-vfbn8 1/1 Running 0 14m  Lets see the endpoints of service\n$ kubectl get ep nginx-svc  Output\nNAME ENDPOINTS AGE nginx-svc 10.10.36.201:80,10.10.36.202:80,10.10.36.203:80 5m40s  Endpoints will be automatically mapped , because when we scale the deployment , the newly created pod will have same label which matches the Service selector.\n"
},
{
	"uri": "/15-k8s_from_sratch/03-certificate-ca/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " PKI Infrastructure We will provision a PKI Infrastructure using CloudFlare\u0026rsquo;s PKI toolkit, cfssl, then use it to bootstrap a Certificate Authority, and generate TLS certificates for the following components: etcd, kube-apiserver, kube-controller-manager, kube-scheduler, kubelet, and kube-proxy.\nCertificate Authority In cryptography, a certificate authority or certification authority (CA) is an entity that issues digital certificates.\n Generate CA default files (To understand the structure of CA and CSR json . We will overwrite this configs in next steps)  $ cfssl print-defaults config \u0026gt; ca-config.json $ cfssl print-defaults csr \u0026gt; ca-csr.json   Modify ca-config and ca-csr to fit your requirement  OR\n Use below commands to create ca-config and ca-csr JSON files  CA Configuration\n$ cat \u0026lt;\u0026lt;EOF \u0026gt;ca-config.json { \u0026quot;signing\u0026quot;: { \u0026quot;default\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;8760h\u0026quot; }, \u0026quot;profiles\u0026quot;: { \u0026quot;kubernetes\u0026quot;: { \u0026quot;expiry\u0026quot;: \u0026quot;8760h\u0026quot;, \u0026quot;usages\u0026quot;: [ \u0026quot;signing\u0026quot;, \u0026quot;key encipherment\u0026quot;, \u0026quot;server auth\u0026quot;, \u0026quot;client auth\u0026quot; ] } } } } EOF  CA CSR\n$ cat \u0026lt;\u0026lt;EOF \u0026gt;ca-csr.json { \u0026quot;CN\u0026quot;: \u0026quot;Kubernetes\u0026quot;, \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;IN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;KL\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;Kubernetes\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;CA\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Kerala\u0026quot; } ] } EOF  $ cfssl gencert -initca ca-csr.json |cfssljson -bare ca   Output  2018/10/01 22:03:14 [INFO] generating a new CA key and certificate from CSR 2018/10/01 22:03:14 [INFO] generate received request 2018/10/01 22:03:14 [INFO] received CSR 2018/10/01 22:03:14 [INFO] generating key: rsa-2048 2018/10/01 22:03:14 [INFO] encoded CSR 2018/10/01 22:03:14 [INFO] signed certificate with serial number 621260968886516247086480084671432552497699065843   ca.pem , ca-key.pem, ca.csr files will be created , but we need only ca.pem and ca-key.pem  $ ls -lrt ca*  -rw-rw-r-- 1 k8s k8s 385 Oct 1 21:53 ca-config.json -rw-rw-r-- 1 k8s k8s 262 Oct 1 21:56 ca-csr.json -rw-rw-r-- 1 k8s k8s 1350 Oct 1 22:03 ca.pem -rw------- 1 k8s k8s 1679 Oct 1 22:03 ca-key.pem -rw-r--r-- 1 k8s k8s 997 Oct 1 22:03 ca.csr  "
},
{
	"uri": "/03-pods/",
	"title": "Pods &amp; Nodes",
	"tags": [],
	"description": "",
	"content": " Chapter 3 Pods \u0026amp; Nodes In this session , we will explore Pods and Nodes.\nWe will also create a Coffee application Pod\n"
},
{
	"uri": "/03-pods/04-nodes/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Nodes In this session , we will explore the node details\nList nodes $ k8s@k8s-master-01:~$ kubectl get nodes  Output\nNAME STATUS ROLES AGE VERSION k8s-master-01 Ready master 38h v1.13.1 k8s-worker-01 Ready \u0026lt;none\u0026gt; 38h v1.13.1  Extended listing $ kubectl get nodes -o wide  Output\nNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME k8s-master-01 Ready master 38h v1.13.1 192.168.56.201 \u0026lt;none\u0026gt; Ubuntu 16.04.5 LTS 4.4.0-131-generic docker://18.9.0 k8s-worker-01 Ready \u0026lt;none\u0026gt; 38h v1.13.1 192.168.56.202 \u0026lt;none\u0026gt; Ubuntu 16.04.5 LTS 4.4.0-131-generic docker://18.9.0 k8s@k8s-master-01:~$  Details on a node $ kubectl describe node k8s-master-01  Output\nName: k8s-master-01 Roles: master Labels: beta.kubernetes.io/arch=amd64 beta.kubernetes.io/os=linux kubernetes.io/hostname=k8s-master-01 node-role.kubernetes.io/master= Annotations: kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock node.alpha.kubernetes.io/ttl: 0 projectcalico.org/IPv4Address: 192.168.56.201/24 volumes.kubernetes.io/controller-managed-attach-detach: true CreationTimestamp: Mon, 31 Dec 2018 02:10:05 +0530 Taints: node-role.kubernetes.io/master:NoSchedule Unschedulable: false Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- MemoryPressure False Tue, 01 Jan 2019 17:01:28 +0530 Mon, 31 Dec 2018 02:10:02 +0530 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure False Tue, 01 Jan 2019 17:01:28 +0530 Mon, 31 Dec 2018 02:10:02 +0530 KubeletHasNoDiskPressure kubelet has no disk pressure PIDPressure False Tue, 01 Jan 2019 17:01:28 +0530 Mon, 31 Dec 2018 02:10:02 +0530 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Tue, 01 Jan 2019 17:01:28 +0530 Mon, 31 Dec 2018 22:59:35 +0530 KubeletReady kubelet is posting ready status. AppArmor enabled Addresses: InternalIP: 192.168.56.201 Hostname: k8s-master-01 Capacity: cpu: 1 ephemeral-storage: 49732324Ki hugepages-2Mi: 0 memory: 2048168Ki pods: 110 Allocatable: cpu: 1 ephemeral-storage: 45833309723 hugepages-2Mi: 0 memory: 1945768Ki pods: 110 System Info: Machine ID: 96cedf74a821722b0df5ee775c291ea2 System UUID: 90E04905-218D-4673-A911-9676A65B07C5 Boot ID: 14201246-ab82-421e-94f6-ff0d8ad3ba54 Kernel Version: 4.4.0-131-generic OS Image: Ubuntu 16.04.5 LTS Operating System: linux Architecture: amd64 Container Runtime Version: docker://18.9.0 Kubelet Version: v1.13.1 Kube-Proxy Version: v1.13.1 PodCIDR: 192.168.0.0/24 Non-terminated Pods: (6 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits AGE --------- ---- ------------ ---------- --------------- ------------- --- kube-system calico-node-nkcrd 250m (25%) 0 (0%) 0 (0%) 0 (0%) 38h kube-system etcd-k8s-master-01 0 (0%) 0 (0%) 0 (0%) 0 (0%) 38h kube-system kube-apiserver-k8s-master-01 250m (25%) 0 (0%) 0 (0%) 0 (0%) 38h kube-system kube-controller-manager-k8s-master-01 200m (20%) 0 (0%) 0 (0%) 0 (0%) 38h kube-system kube-proxy-tzznm 0 (0%) 0 (0%) 0 (0%) 0 (0%) 38h kube-system kube-scheduler-k8s-master-01 100m (10%) 0 (0%) 0 (0%) 0 (0%) 38h Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 800m (80%) 0 (0%) memory 0 (0%) 0 (0%) ephemeral-storage 0 (0%) 0 (0%) Events: \u0026lt;none\u0026gt;  We will discuss more about each of the fields on upcoming sessions. For now lets discuss about Non-terminated Pods field;\nNon-terminated Pods field  Namespace : The namespace which the Pods were running . The pods that we create will by default go to default namespace. Name : Name of the Pod CPU Request : How much CPU resource requested by Pod during startup. CPU Limits : How much CPU the Pod can use. Memory Request : How much memory requested by Pod during startup. Memory Limits : How much memory the Pod can use.  "
},
{
	"uri": "/06-services/04-loadbalancer/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Load Balancer Exposes the service externally using a cloud provider’s load balancer. NodePort and ClusterIP services, to which the external load balancer will route, are automatically created.\nWe will discuss more about this topic later in this training.\n"
},
{
	"uri": "/15-k8s_from_sratch/04-client-and-server-certs/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Client and Server Certificates In this section you will generate client and server certificates for each Kubernetes component and a client certificate for the Kubernetes admin user.\nThe Admin Client Certificate (This will be used for kubectl command) $ { cat \u0026gt; admin-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;admin\u0026quot;, \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;IN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Bangalore\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;system:masters\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;Kubernetes The Hard Way with vBox\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Karnataka\u0026quot; } ] } EOF $ cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ admin-csr.json | cfssljson -bare admin }  Results:\nadmin-key.pem admin.pem  The Kubelet Client Certificates Kubernetes uses a special-purpose authorization mode called Node Authorizer, that specifically authorizes API requests made by Kubelets. In order to be authorized by the Node Authorizer, Kubelets must use a credential that identifies them as being in the system:nodes group, with a username of system:node:\u0026lt;nodeName\u0026gt;. In this section you will create a certificate for each Kubernetes worker node that meets the Node Authorizer requirements.\nGenerate a certificate and private key for each Kubernetes worker node:\n$ for instance in worker-01 worker-02 worker-03; do cat \u0026gt; ${instance}-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;system:node:${instance}\u0026quot;, \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;IN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Bangalore\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;system:masters\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;Kubernetes The Hard Way with vBox\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Karnataka\u0026quot; } ] } EOF IP=$(echo -n ${instance} |tail -c 1 \u0026amp;\u0026amp; echo) EXTERNAL_IP=192.168.78.21${IP} cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=${instance},${EXTERNAL_IP} \\ -profile=kubernetes \\ ${instance}-csr.json | cfssljson -bare ${instance} done  Results:\nworker-01-key.pem worker-01.pem worker-02-key.pem worker-02.pem worker-03-key.pem worker-03.pem  The Controller Manager Client Certificate Generate the kube-controller-manager client certificate and private key:\n{ cat \u0026gt; kube-controller-manager-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;system:kube-controller-manager\u0026quot;, \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;IN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Bangalore\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;system:masters\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;Kubernetes The Hard Way with vBox\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Karnataka\u0026quot; } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager }  Results:\nkube-controller-manager-key.pem kube-controller-manager.pem  The Kube Proxy Client Certificate Generate the kube-proxy client certificate and private key:\n{ cat \u0026gt; kube-proxy-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;system:kube-proxy\u0026quot;, \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;IN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Bangalore\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;system:masters\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;Kubernetes The Hard Way with vBox\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Karnataka\u0026quot; } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-proxy-csr.json | cfssljson -bare kube-proxy }  Results:\nkube-proxy-key.pem kube-proxy.pem  The Scheduler Client Certificate Generate the kube-scheduler client certificate and private key:\n{ cat \u0026gt; kube-scheduler-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;system:kube-scheduler\u0026quot;, \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;IN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Bangalore\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;system:masters\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;Kubernetes The Hard Way with vBox\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Karnataka\u0026quot; } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ kube-scheduler-csr.json | cfssljson -bare kube-scheduler }  Results:\nkube-scheduler-key.pem kube-scheduler.pem  The Kubernetes API Server Certificate The kubernetes-the-hard-way static IP address will be included in the list of subject alternative names for the Kubernetes API Server certificate. This will ensure the certificate can be validated by remote clients.\nGenerate the Kubernetes API Server certificate and private key:\n{ KUBERNETES_ADDRESS=172.168.0.1,192.168.78.201,192.168.78.202,192.168.78.203,192.168.78.225,192.168.78.226,192.168.78.220 cat \u0026gt; kubernetes-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;kubernetes\u0026quot;, \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;IN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Bangalore\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;system:masters\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;Kubernetes The Hard Way with vBox\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Karnataka\u0026quot; } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -hostname=${KUBERNETES_ADDRESS},127.0.0.1,kubernetes.default \\ -profile=kubernetes \\ kubernetes-csr.json | cfssljson -bare kubernetes }  Results:\nkubernetes-key.pem kubernetes.pem  The Service Account Key Pair The Kubernetes Controller Manager leverages a key pair to generate and sign service account tokens as describe in the managing service accounts documentation.\nGenerate the service-account certificate and private key:\n{ cat \u0026gt; service-account-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;service-accounts\u0026quot;, \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;IN\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Bangalore\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;system:masters\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;Kubernetes The Hard Way with vBox\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Karnataka\u0026quot; } ] } EOF cfssl gencert \\ -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes \\ service-account-csr.json | cfssljson -bare service-account }  Results:\nservice-account-key.pem service-account.pem  Distribute the Client and Server Certificates Copy the appropriate certificates and private keys to each worker instance:\nfor instance in worker-01 worker-02 worker-03; do scp ca.pem ${instance}-key.pem ${instance}.pem ${instance}:~/ done  Copy the appropriate certificates and private keys to each controller instance:\nfor instance in controller-01 controller-02 controller-03; do scp ca.pem ca-key.pem kubernetes-key.pem kubernetes.pem \\ service-account-key.pem service-account.pem ${instance}:~/ done   The kube-proxy, kube-controller-manager, kube-scheduler, and kubelet client certificates will be used to generate client authentication configuration files in the next lab.\n "
},
{
	"uri": "/04-labels_and_annotations/",
	"title": "Labels &amp; Annotations",
	"tags": [],
	"description": "",
	"content": " Chapter 4 Labels \u0026amp; Annotations In this session , we will discuss the role of Labels and Annotations , also its role in fundamental k8s design.\n"
},
{
	"uri": "/01-introduction/01-linux-kernel/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Linux Kernel Architecture At the top is the user, or application, space. This is where the user applications are executed. Below the user space is the kernel space. Here, the Linux kernel exists.\nThere is also the GNU C Library (glibc). This provides the system call interface that connects to the kernel and provides the mechanism to transition between the user-space application and the kernel. This is important because the kernel and user application occupy different protected address spaces. And while each user-space process occupies its own virtual address space, the kernel occupies a single address space.\nThe Linux kernel can be further divided into three gross levels. At the top is the system call interface, which implements the basic functions such as read and write. Below the system call interface is the kernel code, which can be more accurately defined as the architecture-independent kernel code. This code is common to all of the processor architectures supported by Linux. Below this is the architecture-dependent code, which forms what is more commonly called a BSP (Board Support Package). This code serves as the processor and platform-specific code for the given architecture.\nThe Linux kernel implements a number of important architectural attributes. At a high level, and at lower levels, the kernel is layered into a number of distinct subsystems.\n"
},
{
	"uri": "/02-installation/01-vbox-install/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " VirtualBox  Download VBox installer Download VBox Extension Pack\n https://download.virtualbox.org/virtualbox/LATEST.TXT https://download.virtualbox.org/virtualbox//  Procedure is available in below link\n https://www.wikihow.com/Install-VirtualBox   "
},
{
	"uri": "/03-pods/05-namespaces/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Namespaces What is a namespace We have see namespaces in Linux , which ideally isolates objects and here also the concept is same but serves a different purpose. Suppose you have two departments in you organization and both departments have application which needs more fine grained control. We can use namespaces to separate the workload of each departments.\nBy default kubernetes will have three namespace\nList namespace $ kubectl get ns NAME STATUS AGE default Active 39h kube-public Active 39h kube-system Active 39h  default : All Pods that we manually create will go to this namespace (There are ways to change it , but for now that is what it is). kube-public : All common workloads can be assigned to this namespace . Most of the time no-one use it. kube-system : Kubernetes specific Pods will be running on this namespace\nList Pods in kube-system namespace $ kubectl get pods --namespace=kube-system NAME READY STATUS RESTARTS AGE calico-node-n99tb 2/2 Running 0 38h calico-node-nkcrd 2/2 Running 0 38h coredns-86c58d9df4-4c22l 1/1 Running 0 39h coredns-86c58d9df4-b49c2 1/1 Running 0 39h etcd-k8s-master-01 1/1 Running 0 39h kube-apiserver-k8s-master-01 1/1 Running 0 39h kube-controller-manager-k8s-master-01 1/1 Running 0 39h kube-proxy-s6hc4 1/1 Running 0 38h kube-proxy-tzznm 1/1 Running 0 39h kube-scheduler-k8s-master-01 1/1 Running 0 39h  As you can see , there are many Pods running in kube-system namespace All these Pods were running with one or mode containers If you see the calico-node-n99tb pod , the READY says 2\u0026frasl;2 , which means two containers were running fine in this Pod\nList all resources in a namespace k8s@k8s-master-01:~$ kubectl get all -n kube-system NAME READY STATUS RESTARTS AGE pod/calico-node-kr5xg 2/2 Running 0 13m pod/calico-node-lcpbw 2/2 Running 0 13m pod/coredns-86c58d9df4-h8pjr 1/1 Running 6 26m pod/coredns-86c58d9df4-xj24c 1/1 Running 6 26m pod/etcd-k8s-master-01 1/1 Running 0 26m pod/kube-apiserver-k8s-master-01 1/1 Running 0 26m pod/kube-controller-manager-k8s-master-01 1/1 Running 0 26m pod/kube-proxy-fl7rj 1/1 Running 0 26m pod/kube-proxy-q6w9l 1/1 Running 0 26m pod/kube-scheduler-k8s-master-01 1/1 Running 0 26m NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/calico-typha ClusterIP 172.16.244.140 \u0026lt;none\u0026gt; 5473/TCP 13m service/kube-dns ClusterIP 172.16.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 27m NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE daemonset.apps/calico-node 2 2 2 2 2 beta.kubernetes.io/os=linux 13m daemonset.apps/kube-proxy 2 2 2 2 2 \u0026lt;none\u0026gt; 27m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/calico-typha 0/0 0 0 13m deployment.apps/coredns 2/2 2 2 27m NAME DESIRED CURRENT READY AGE replicaset.apps/calico-typha-5fc4874c76 0 0 0 13m replicaset.apps/coredns-86c58d9df4 2 2 2 26m k8s@k8s-master-01:~$  "
},
{
	"uri": "/06-services/05-endpoints/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Pods behind a service. Lets describe the service to see how the mapping of Pods works in a service object.\n(Yes , we are slowly moving from general wordings to pure kubernetes terms)\n$ kubectl describe service coffee Name: coffee Namespace: default Labels: run=coffee Annotations: \u0026lt;none\u0026gt; Selector: run=coffee Type: NodePort IP: 192.168.10.86 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 9090/TCP NodePort: \u0026lt;unset\u0026gt; 30391/TCP Endpoints: 10.10.1.13:9090 Session Affinity: None External Traffic Policy: Cluster  Here the label run=coffee is the one which creates the mapping from service to Pod.\nAny pod with label run=coffee will be mapped under this service.\nThose mappings are called Endpoints.\nLets see the endpoints of service coffee\n$ kubectl get endpoints coffee NAME ENDPOINTS AGE coffee 10.10.1.13:9090 3h48m  As of now only one pod endpoint is mapped under this service.\nlets create one more Pod with same label and see how it affects endpoints.\n$ kubectl run coffee01 --image=ansilh/demo-coffee --restart=Never --labels=run=coffee  Now we have one more Pod\n$ kubectl get pods NAME READY STATUS RESTARTS AGE coffee 1/1 Running 0 15h coffee01 1/1 Running 0 6s  Lets check the endpoint\n$ kubectl get endpoints coffee NAME ENDPOINTS AGE coffee 10.10.1.13:9090,10.10.1.19:9090 3h51m  Now we have two Pod endpoints mapped to this service. So the requests comes to coffee service will be served from these pods in a round robin fashion.\n"
},
{
	"uri": "/05-yaml_primer/",
	"title": "YAML Crash course",
	"tags": [],
	"description": "",
	"content": " Chapter 5 YAML crash course In this session we will learn k8s YAML specification and object types. We will cover only k8s dependent YAML specification\n"
},
{
	"uri": "/01-introduction/02-namespaces/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Namespaces in Linux Namespaces are a feature of the Linux kernel that partitions kernel resources such that one set of processes sees one set of resources while another set of processes sees a different set of resources. The feature works by having the same name space for these resources in the various sets of processes, but those names referring to distinct resources. Examples of resource names that can exist in multiple spaces, so that the named resources are partitioned, are process IDs, hostnames, user IDs, file names, and some names associated with network access, and interprocess communication.\nNamespaces are a fundamental aspect of containers on Linux.\n   Namespace Constant Isolates     Cgroup CLONE_NEWCGROUP Cgroup root directory   IPC CLONE_NEWIPC System V IPC, POSIX message queues   Network CLONE_NEWNET Network devices, stacks, ports, etc.   Mount CLONE_NEWNS Mount points   PID CLONE_NEWPID Process IDs   User CLONE_NEWUSER User and group IDs   UTS CLONE_NEWUTS Hostname and NIS domain name    The kernel assigns each process a symbolic link per namespace kind in /proc/\u0026lt;pid\u0026gt;/ns/. The inode number pointed to by this symlink is the same for each process in this namespace. This uniquely identifies each namespace by the inode number pointed to by one of its symlinks.\nReading the symlink via readlink returns a string containing the namespace kind name and the inode number of the namespace.\n"
},
{
	"uri": "/02-installation/02-nw-setup/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " VirtualBox network configuration  Create HostOnly network ( Default will be 192.168.56.0/24)  Open Virtual Box Got to menu and navigate to File -\u0026gt;Host Network Manager Then click \u0026ldquo;Create\u0026rdquo; This will create a Host-Only Network.   DHCP should be disabled on this network.\nInternet access is needed on all VMs (for downloading needed binaries).\nMake sure you can see the NAT network.(If not , create one).\n   VBox Host Networking      HostOnly 192.168.56.0/24   NAT VBOX Defined    "
},
{
	"uri": "/03-pods/06-self-healing-readiness/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Self Healing - Readiness Readiness Probe We have seen that our coffee application was listening on port 9090. Lets assume that the application is not coming up but Pod status showing running. Everyone will think that application is up. You entire application stack might get affected because of this.\nSo here comes the question , \u0026ldquo;How can I make sure my application is started, not just the Pod ?\u0026rdquo;\nHere we can use Pod spec, Readiness probe.\nOfficial detention of readinessProbe is , \u0026ldquo;Periodic probe of container service readiness\u0026rdquo;.\nLets rewrite the Pod specification of Coffee App and add a readiness Probe.\n$ vi pod-readiness.yaml  apiVersion: v1 kind: Pod metadata: name: coffee-app spec: containers: - image: ansilh/demo-coffee name: coffee readinessProbe: initialDelaySeconds: 10 httpGet: port: 9090  Apply Yaml $ kubectl apply -f pod-readiness.yaml pod/coffee-app created  Verify Pod status Try to identify the difference.\n$ kubectl get pods NAME READY STATUS RESTARTS AGE coffee-app 0/1 ContainerCreating 0 3s $ kubectl get pods NAME READY STATUS RESTARTS AGE coffee-app 0/1 Running 0 25s $ kubectl get pods NAME READY STATUS RESTARTS AGE coffee-app 1/1 Running 0 32s  Delete the Pod Yes ,we can delete the objects using the same yaml which we used to create/apply it\n$ kubectl delete -f pod-readiness.yaml pod \u0026quot;coffee-app\u0026quot; deleted $  Probe Tuning. failureThreshold \u0026lt;integer\u0026gt; Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1. initialDelaySeconds \u0026lt;integer\u0026gt; Number of seconds after the container has started before liveness probes are initiated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes periodSeconds \u0026lt;integer\u0026gt; How often (in seconds) to perform the probe. Default to 10 seconds. Minimum value is 1. timeoutSeconds \u0026lt;integer\u0026gt; Number of seconds after which the probe times out. Defaults to 1 second. Minimum value is 1. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes  "
},
{
	"uri": "/06-services/",
	"title": "Services",
	"tags": [],
	"description": "",
	"content": " Chapter 6 Services and Service Discovery In this session we will solve the maze "
},
{
	"uri": "/01-introduction/03-cgroups/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " CGroups cgroups (abbreviated from control groups) is a Linux kernel feature that limits, accounts for, and isolates the resource usage (CPU, memory, disk I/O, network, etc.) of a collection of processes.\nResource limiting groups can be set to not exceed a configured memory limit\nPrioritization Some groups may get a larger share of CPU utilization or disk I/O throughput\nAccounting Measures a group\u0026rsquo;s resource usage, which may be used\nControl Freezing groups of processes, their checkpointing and restarting\nYou can read and explore more about cGroups in this post\n"
},
{
	"uri": "/02-installation/03-ubuntu-install/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Ubuntu installation  Download Ubuntu 16.04 ISO http://releases.ubuntu.com/16.04/ubuntu-16.04.5-server-amd64.iso  Create a template VM which will be used to clone all needed VMs\n You need at least 50GB free space to host all VMs All VMs will be placed in a directory called (Don\u0026rsquo;t create these manually now!) DRIVE_NAME:/VMs/ (Replace DRIVE_NAME with a mount point or Driver name) Install Ubuntu 16.04 with latest patches VM configuration\n VM Name : k8s-master-01 Memory : 2 GB CPU : 2 Disk : 100GB HostOnly interface : 1 (ref. step 1). NAT network interface : 1   By default , NAT will be the first in network adapter order , change it. NAT interface should be the second interface and Host-Only should be the first one\n\r Install Ubuntu on this VM and go ahead with all default options\n When asked, provide user name k8s and set password Make sure to select the NAT interface as primary during installation. Select below in Software Selection screen Manual Software Selection OpenSSH Server  After restart , make sure NAT interface is up\n Login to the template VM with user k8s and execute below commands to install latest patches.\n  $ sudo apt-get update $ sudo apt-get upgrade   Poweroff template VM  $ sudo poweroff  Clone VM You may use VirtualBox GUI to create a full clone - Preferred You can use below commands to clone a VM - Execute it at your own risk ;)\n Open CMD and execute below commands to create all needed VMs. You can replace the value of DRIVER_NAME with a drive which is having enough free space (~50GB) Windows  set DRIVE_NAME=D cd C:\\Program Files\\Oracle\\VirtualBox VBoxManage.exe clonevm \u0026quot;k8s-master-01\u0026quot; --name \u0026quot;k8s-worker-01\u0026quot; --groups \u0026quot;/K8S Training\u0026quot; --basefolder \u0026quot;%DRIVE_NAME%:\\VMs\u0026quot; --register   Mac or Linux (Need to test)  DRIVE_NAME=${HOME} VBoxManage clonevm \u0026quot;k8s-master-01\u0026quot; --name \u0026quot;k8s-worker-01\u0026quot; --groups \u0026quot;/K8S Training\u0026quot; --basefolder ${DRIVE_NAME}/VMs\u0026quot; --register  Start VMs one by one and perform below Execute below steps on both master and worker nodes  Assign IP address and make sure it comes up at boot time.  $ sudo systemctl stop networking $ sudo vi /etc/network/interfaces  auto enp0s3 #\u0026lt;-- Make sure to use HostOnly interface (it can also be enp0s8) iface enp0s3 inet static address 192.168.56.X #\u0026lt;--- Replace X with corresponding IP octet netmask 255.255.255.0  $ sudo systemctl restart networking  You may access the VM using the IP via SSH and can complete all remaining steps from that session (for copy paste :) )\n\r- Change Host name\nExecute below steps only on worker node $ HOST_NAME=\u0026lt;host name\u0026gt; # \u0026lt;--- Replace \u0026lt;host name\u0026gt; with corresponding one  $ sudo hostnamectl set-hostname ${HOST_NAME} --static --transient   Regenrate SSH Keys  $ sudo /bin/rm -v /etc/ssh/ssh_host_* $ sudo dpkg-reconfigure openssh-server   Change iSCSI initiator IQN  $ sudo vi /etc/iscsi/initiatorname.iscsi  InitiatorName=iqn.1993-08.org.debian:01:HOST_NAME #\u0026lt;--- Append HostName to have unique iscsi iqn   Change Machine UUID  $ sudo rm /etc/machine-id /var/lib/dbus/machine-id $ sudo systemd-machine-id-setup  Execute below steps on both master and worker nodes  Remove 127.0.1.1 entry from /etc/hosts\n Add needed entries in /etc/hosts\n  $ sudo bash -c \u0026quot;cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt;/etc/hosts 192.168.56.201 k8s-master-01 192.168.56.202 k8s-worker-01 EOF\u0026quot;   Add public DNS incase the local one is not responding in NAT  $ sudo bash -c \u0026quot;cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt;/etc/resolvconf/resolv.conf.d/tail nameserver 8.8.8.8 EOF\u0026quot;   Disable swap by commenting out swap_1 LV  $ sudo vi /etc/fstab  # /dev/mapper/k8s--master--01--vg-swap_1 none swap sw 0 0   Reboot VMs  $ sudo reboot  Do a ping test to make sure both VMs can reach each other.\n\r"
},
{
	"uri": "/03-pods/07-self-healing-liveness/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Self Healing - Liveness Liveness Probe Lets assume the application failed after readiness probe execution completes Again we are back to service unavailability\nTo avoid this , we need a liveness check which will do a periodic health check after Pod start running or readiness probe completes.\nLets rewrite the Pod specification of Coffee App and add a liveness Probe.\n$ vi pod-liveiness.yaml  apiVersion: v1 kind: Pod metadata: name: coffee-app spec: containers: - image: ansilh/demo-coffee name: coffee readinessProbe: initialDelaySeconds: 10 httpGet: port: 9090 livenessProbe: periodSeconds: 5 httpGet: port: 9090  Create Pod $ kubectl create -f pod-liveness.yaml  "
},
{
	"uri": "/05-yaml_primer/01-structure/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " K8S YAML structure What is YAML Yet Another Markup Language\nKubernetes YAML have below structure\napiVersion: kind: metadata: spec:  apiVersion: Kubernetes have different versions of API for each objects. We discuss about API in detail in upcoming sessions. For now , lets keep it simple as possible.\nPod is one of the kind of object which is part of core v1 API So for a Pod, we usually see apiVersion: v1\nkind: As explained above we specify the kind of API object with kind: field.\nmetadata: We have seen the use of metadata earlier.\nAs the name implies , we usually store name of object and labels in metadata field.\nspec: Object specification will go hear. The specification will depend on the kind and apiVersion we use\nExploring Pod spec Lets write a Pod specification YAML\napiVersion: v1 kind: Pod metadata: name: coffee-app01 labels: app: frontend run: coffee-app01 spec: containers: - name: demo-coffee image: ansilh/demo-coffee  In above specification , you can see that we have specified name and labels in matadata field.\nThe spec starts with cotainer field and we have added a container specification under it.\nYou might be wondering , how can we memories all these options. In reality , you don\u0026rsquo;t have to.\nWe will discuss about it in next session.\n"
},
{
	"uri": "/05-yaml_primer/02-explore/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Exploring Object Specs So lets discuss about a new command kubectl explain so that we don\u0026rsquo;t have to remember all YAML specs of kubernetes objects.\nWith kubectl explain subcommand , you can see the specification of each objects and can use that as a reference to write your YAML files.\nFist level spec We will use kubectl explain Pod command to see the specifications of a Pod YAML.\n$ kubectl explain Pod  Output\nubuntu@k8s-master-01:~$ kubectl explain pod KIND: Pod VERSION: v1 DESCRIPTION: Pod is a collection of containers that can run on a host. This resource is created by clients and scheduled onto hosts. FIELDS: apiVersion \u0026lt;string\u0026gt; APIVersion defines the versioned schema of this representation of an object. Servers should convert recognized schemas to the latest internal value, and may reject unrecognized values. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#resources kind \u0026lt;string\u0026gt; Kind is a string value representing the REST resource this object represents. Servers may infer this from the endpoint the client submits requests to. Cannot be updated. In CamelCase. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#types-kinds metadata \u0026lt;Object\u0026gt; Standard object's metadata. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata spec \u0026lt;Object\u0026gt; Specification of the desired behavior of the pod. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status status \u0026lt;Object\u0026gt; Most recently observed status of the pod. This data may not be up to date. Populated by the system. Read-only. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status ubuntu@k8s-master-01:~$  As we discussed earlier , the specification is very familiar.\nFiled status is readonly and its system populated , so we don\u0026rsquo;t have to write anything for status.\nExploring inner fields If we want to see the fields available in spec , then execute below command.\n$ kubectl explain pod.spec  KIND: Pod VERSION: v1 RESOURCE: spec \u0026lt;Object\u0026gt; DESCRIPTION: Specification of the desired behavior of the pod. More info: https://git.k8s.io/community/contributors/devel/api-conventions.md#spec-and-status PodSpec is a description of a pod. FIELDS: ... containers \u0026lt;[]Object\u0026gt; -required- List of containers belonging to the pod. Containers cannot currently be added or removed. There must be at least one container in a Pod. Cannot be updated. ...  How easy is that.\nAs you can see in spec the containers filed is -required- which indicates that this filed is mandatory.\n\u0026lt;[]Object\u0026gt; indicates that its an array of objects , which means , you can put more than one element under containers\nThat make sense , because the Pod may contain more than one container.\nIn YAML we can use - infront of a filed to mark it as an array element.\nLets take a look at the YAML that we wrote earlier\napiVersion: v1 kind: Pod metadata: name: coffee-app01 labels: app: frontend run: coffee-app01 spec: containers: - name: demo-coffee image: ansilh/demo-coffee  There is a - under the fist filed of the containers. If we say that in words ; \u0026ldquo;containers is an array object which contains one array element with filed name and image\u0026ldquo;\nIf you want to add one more container in Pod , we will add one more array element with needed values.\napiVersion: v1 kind: Pod metadata: name: coffee-app01 labels: app: frontend run: coffee-app01 spec: containers: - name: demo-coffee image: ansilh/demo-coffee - name: demo-tea image: ansilh/demo-tea  Now the Pod have two containers .\nHow I know the containers array element need name and image ?\nWe will use explain command to get those details.\n$ kubectl explain pod.spec.containers  Snipped Output\n... name \u0026lt;string\u0026gt; -required- Name of the container specified as a DNS_LABEL. Each container in a pod must have a unique name (DNS_LABEL). Cannot be updated. image \u0026lt;string\u0026gt; Docker image name ...  As you can see , name and image are of type string which means , you have to provide a string value to it.\n"
},
{
	"uri": "/07-multi_container_pod/",
	"title": "Multi-Container Pods",
	"tags": [],
	"description": "",
	"content": " Chapter 7 Multi-Container Pods In this session we will create Pods with more than one containers and few additional features in k8s.\n"
},
{
	"uri": "/01-introduction/04-containers/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Container From Scratch Using namespaces , we can start a process which will be completely isolated from other processes running in the system.\nCreate root File System Create directory to store rootfs contents $ mkdir -p /root/busybox/rootfs $ CONTAINER_ROOT=/root/busybox/rootfs $ cd ${CONTAINER_ROOT}  Download busybox binary $ wget https://busybox.net/downloads/binaries/1.28.1-defconfig-multiarch/busybox-x86_64  Create needed directories and symlinks $ mv busybox-x86_64 busybox $ chmod 755 busybox $ mkdir bin $ mkdir proc $ mkdir sys $ mkdir tmp $ for i in $(./busybox --list) do ln -s /busybox bin/$i done  Start Container Start a shell in new contianer $ unshare --mount --uts --ipc --net --pid --fork --user --map-root-user chroot ${CONTAINER_ROOT} /bin/sh  Mount essential kernel structures $ mount -t proc none /proc $ mount -t sysfs none /sys $ mount -t tmpfs none /tmp  Configure networking From Host system , create a veth pair and then map that to container $ sudo ip link add vethlocal type veth peer name vethNS $ sudo ip link set vethlocal up $ sudo ip link set vethNS up $ sudo ps -ef |grep '/bin/sh' $ sudo ip link set vethNS netns \u0026lt;pid of /bin/sh\u0026gt;  From container , execute ip link "
},
{
	"uri": "/02-installation/04-docker-install/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Install Docker In this session, we will install and setup docker in a simple and easy way on Ubuntu 16.04.\n Add gpg key to aptitude  $ curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -   Add repository  $ sudo add-apt-repository \u0026quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\u0026quot;   Refresh repository  $ sudo apt-get update   Verify whether docker is available in repo or not  $ sudo apt-cache policy docker-ce  docker-ce: Installed: (none) Candidate: 5:18.09.0~3-0~ubuntu-xenial Version table: 5:18.09.0~3-0~ubuntu-xenial 500 ...   Install docker  $ sudo apt-get install -y docker-ce   Make sure docker is running  $ sudo systemctl status docker  ● docker.service - Docker Application Container Engine Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled) Active: active (running) since Wed 2018-12-26 17:14:59 UTC; 4min 27s ago Docs: https://docs.docker.com Main PID: 1191 (dockerd) Tasks: 10 Memory: 76.4M CPU: 625ms CGroup: /system.slice/docker.service └─1191 /usr/bin/dockerd -H unix:// ...   Add user to docker group so that this user can execute docker commands.  $ sudo usermod -aG docker ${USER}  Logout the session and login again to refresh the group membership.\n\r Verify docker by executing info command.  $ docker info |grep 'Server Version'  Server Version: 18.09.0  "
},
{
	"uri": "/03-pods/08-request-limits/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Resource Allocation - CPU and Memory allocation for containers. Limits We can limit the CPU and Memory usage of a container so that one\nLets create the coffee Pod again with CPU and Memory limits\napiVersion: v1 kind: Pod metadata: labels: name: coffee-limits spec: containers: - image: ansilh/demo-coffee name: coffee resources: limits: CPU: 100m Memory: 123Mi  Resulting container will be allowed to use 100 millicores and 123 mebibyte (~128 Megabytes)\nCPU One CPU core is equivalent to 1000m (one thousand millicpu or one thousand millicores) CPU is always expressed as an absolute quantity, never as a relative quantity; 0.1 is the same amount of CPU on a single-core, dual-core, or 48-core machine\nMemory You can express memory as a plain integer or as a fixed-point integer using one of these suffixes: E, P, T, G, M, K. You can also use the power-of-two equivalents: Ei, Pi, Ti, Gi, Mi, Ki. For example, the following represent roughly the same value:\n128974848, 129e6, 129M, 123Mi  Mebibyte vs Megabyte 1 Megabyte (MB) = (1000)^2 bytes = 1000000 bytes. 1 Mebibyte (MiB) = (1024)^2 bytes = 1048576 bytes.  Requests We can request a specific amount of CPU and Memory when the container starts up.\nSuppose if the Java application need at least 128MB of memory during startup , we can use resource request in Pod spec.\nThis will help the scheduler to select a node with enough memory.\nRequest also can be made of CPU as well.\nLets modify the Pod spec and add request\napiVersion: v1 kind: Pod metadata: labels: name: coffee-limits spec: containers: - image: ansilh/demo-coffee name: coffee resources: requests: CPU: 100m Memory: 123Mi limits: CPU: 200m Memory: 244Mi  Extra Once you complete the training , you can visit below URLs to understand storage and network limits.\nStorage Limit\nNetwork bandwidth usage\n"
},
{
	"uri": "/08-api_reference/",
	"title": "API reference",
	"tags": [],
	"description": "",
	"content": " Chapter 8 API reference In this session we will explore k8s API , Objects and Versioning\n"
},
{
	"uri": "/01-introduction/07-network-plugins/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Container networking - Demo We need to access the container from outside world and the container running on different hosts have to communicate each other.\nHere we will see how can we do it with bridging.\nTraditional networking Create a veth pair on Host. $ sudo ip link add veth0 type veth peer name veth1 $ sudo ip link show  Create a network namespace $ sudo ip netns add bash-nw-namespace $ sudo ip netns show  Connect one end to namespace $ sudo ip link set veth1 netns bash-nw-namespace $ sudo ip link list  Resulting network Create a Bridge interface $ sudo brctl addbr cbr0  Add an external interface to bridge $ sudo brctl addif cbr0 enp0s9 $ sudo brctl show  Connect other end to a switch $ sudo brctl addif cbr0 veth0 $ sudo brctl show  Resulting network Assign IP to interface $ sudo ip netns exec bash-nw-namespace bash $ sudo ip addr add 192.168.56.10/24 dev veth1 $ sudo ip link set lo up $ sudo ip link set dev veth1 up  Access container IP from outside Like bridging , we can opt other networking solutions.\nLater we will see how Weave Network and Calico plugins works. You may read bit more on Docker networking basics on below blog post\nDocker networking\n"
},
{
	"uri": "/02-installation/05-golang-setup/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Setup Golang  Download Golang tarball  $ curl -O https://dl.google.com/go/go1.11.4.linux-amd64.tar.gz   Extract the contents  $ tar -xvf go1.11.4.linux-amd64.tar.gz   Move the contents to /usr/local directory  $ sudo mv go /usr/local/   Add the environmental variable GOPATH to .profile  cat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt;~/.profile export GOPATH=\\$HOME/work export PATH=\\$PATH:/usr/local/go/bin:\\$GOPATH/bin EOF   Create the work directory  $ mkdir $HOME/work   Load the profile  $ source ~/.profile   Verify Golang setup  $ go version  go version go1.11.4 linux/amd64   Create a directory tree to map to a github repository  $ mkdir -p $GOPATH/src/github.com/ansilh/golang-demo   Create a hello world golang program  $ vi $GOPATH/src/github.com/ansilh/golang-demo/main.go   Paste below code\n  package main import \u0026quot;fmt\u0026quot; func main(){ fmt.Println(\u0026quot;Hello World.!\u0026quot;) }   Build and install the program  go install github.com/ansilh/golang-demo   Execute the program to see the output  $ golang-demo  Hello World.!  "
},
{
	"uri": "/09-configmaps_and_secrets/",
	"title": "ConfigMaps and Secrets",
	"tags": [],
	"description": "",
	"content": " Chapter 9 ConfigMaps and Secrets In this session we will explore the need of ConfigMaps and Secrets and its usage.\n"
},
{
	"uri": "/01-introduction/05-docker/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " What is Docker Docker is a tool designed to make it easier to create, deploy, and run applications by using containers. Containers allow a developer to package up an application with all of the parts it needs, such as libraries and other dependencies, and ship it all out as one package.\nIn a way, Docker is a bit like a virtual machine. But unlike a virtual machine, rather than creating a whole virtual operating system, Docker allows applications to use the same Linux kernel as the system that they\u0026rsquo;re running on and only requires applications be shipped with things not already running on the host computer. This gives a significant performance boost and reduces the size of the application.\n"
},
{
	"uri": "/02-installation/06-demo-webapp/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Build a Demo WebApp  Create a directory for the demo app.  $ mkdir -p ${GOPATH}/src/github.com/ansilh/demo-webapp   Create demo-webapp.go file  $ vi ${GOPATH}/src/github.com/ansilh/demo-webapp/demo-webapp.go  package main import ( \u0026quot;fmt\u0026quot; \u0026quot;net/http\u0026quot; \u0026quot;log\u0026quot; ) func demoDefault(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026quot;404 - Page not found - This is a dummy default backend\u0026quot;) // send data to client side } func main() { http.HandleFunc(\u0026quot;/\u0026quot;, demoDefault) // set router err := http.ListenAndServe(\u0026quot;:9090\u0026quot;, nil) // set listen port if err != nil { log.Fatal(\u0026quot;ListenAndServe: \u0026quot;, err) } }   Build a static binary  $ cd $GOPATH/src/github.com/ansilh/demo-webapp $ CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -a -installsuffix cgo -ldflags=\u0026quot;-w -s\u0026quot; -o $GOPATH/bin/demo-webapp   Execute the program  $ demo-webapp  Open the browser and check if you can see the response using IP:9090 If you see the output “404 – Page not found – This is a dummy default backend” indicates that the program is working\nPress Ctrl+c to terminate the program\n"
},
{
	"uri": "/10-deployments/",
	"title": "Deployments",
	"tags": [],
	"description": "",
	"content": " Chapter 10 Deployments \u0026amp; ReplicaSets We will not run individual Pods in K8S cluster for running the workload. Instead , we will use more abstract objects like Deployments.\n"
},
{
	"uri": "/01-introduction/06-kubernetes/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Kubernetes Pet vs Cattle.\nIn the pets service model, each pet server is given a loving names like zeus, ares, hades, poseidon, and athena. They are “unique, lovingly hand-raised, and cared for, and when they get sick, you nurse them back to health”. You scale these up by making them bigger, and when they are unavailable, everyone notices.\nIn the cattle service model, the servers are given identification numbers like web-01, web-02, web-03, web-04, and web-05, much the same way cattle are given numbers tagged to their ear. Each server is “almost identical to each other” and “when one gets sick, you replace it with another one”. You scale these by creating more of them, and when one is unavailable, no one notices.\nKubernetes is a portable, extensible open-source platform for managing containerized workloads and services, that facilitates both declarative configuration and automation. It has a large, rapidly growing ecosystem. Kubernetes services, support, and tools are widely available.\nGoogle open-sourced the Kubernetes project in 2014. Kubernetes builds upon a decade and a half of experience that Google has with running production workloads at scale, combined with best-of-breed ideas and practices from the community\nRead More here\nKubernetes Architecture Container runtime Docker , rkt , containerd or any OCI compliant runtime which will download image , configures network , mount volumes and assist container life cycle management.\nkubelet Responsible for instructing container runtime to start , stop or modify a container\nkube-proxy Manage service IPs and iptables rules\nkube-apiserver API server interacts with all other components in cluster All client interactions will happen via API server\nkube-scheduler Responsible for scheduling workload on minions or worker nodes based on resource constraints\nkube-controller-manager Responsible for monitoring different containers in reconciliation loop Will discuss more about different controllers later in this course\netcd Persistent store where we store all configurations and cluster state\ncloud-controller-manager Cloud vendor specific controller and cloud vendor is Responsible to develop this program\n"
},
{
	"uri": "/02-installation/07-build-docker-image/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Build Docker image using Dockerfile Create a Docker Hub account\n Let’s create a directory to store the Dockerfile  $ mkdir ~/demo-webapp   Copy the pre-built program  $ cp $GOPATH/bin/demo-webapp ~/demo-webapp/   Create a Dockerfile.  $ cd ~/demo-webapp/ $ vi Dockerfile  FROM scratch LABEL maintainer=\u0026quot;Ansil H\u0026quot; LABEL email=\u0026quot;ansilh@gmail.com\u0026quot; COPY demo-webapp / CMD [\u0026quot;/demo-webapp\u0026quot;]   Build the docker image  $ sudo docker build -t \u0026lt;docker login name\u0026gt;/demo-webapp . Eg:- $ sudo docker build -t ansilh/demo-webapp .   Login to Docker Hub using your credentials  $ docker login   Push image to Docker hub  $ docker push \u0026lt;docker login name\u0026gt;/demo-webapp Eg:- $ docker push ansilh/demo-webapp  Congratulations ! . Now the image you built is available in Docker Hub and we can use this image to run containers in upcoming sessions\n"
},
{
	"uri": "/11-daemonsets/",
	"title": "Deployments",
	"tags": [],
	"description": "",
	"content": " Chapter 11 DaemonSets With DaemonSets , we can run one pod on each nodes or minions. This kind of pods will act more or less like an agent.\n"
},
{
	"uri": "/02-installation/08-run-docker/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Container management using Docker Start a Container  Here we map port 80 of host to port 9090 of cotainer Verify application from browser Press Ctrl+c to exit container  $ docker run -p 80:9090 ansilh/demo-webapp   Start a Container in detach mode  $ docker run -d -p 80:9090 ansilh/demo-webapp   List Container  $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4c8364e0d031 ansilh/demo-webapp \u0026quot;/demo-webapp\u0026quot; 11 seconds ago Up 10 seconds 0.0.0.0:80-\u0026gt;9090/tcp zen_gauss   List all containers including stopped containers  $ docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4c8364e0d031 ansilh/demo-webapp \u0026quot;/demo-webapp\u0026quot; 2 minutes ago Up 2 minutes 0.0.0.0:80-\u0026gt;9090/tcp zen_gauss acb01851c20a ansilh/demo-webapp \u0026quot;/demo-webapp\u0026quot; 2 minutes ago Exited (2) 2 minutes ago condescending_antonelli   List resource usage (Press Ctrl+c to exit)  $ docker stats zen_gauss   Stop Container  $ docker stop zen_gauss   List images  $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE ansilh/demo-webapp latest b7c5e17ae85e 8 minutes ago 4.81MB   Remove containers  $ docker rm zen_gauss   Delete images  $ docker rmi ansilh/demo-webapp  "
},
{
	"uri": "/02-installation/09-install-kubeadm/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Install kubelet , kubeadm and kubectl Verify the MAC address and product_uuid are unique for every node (ip link or ifconfig -a and sudo cat /sys/class/dmi/id/product_uuid)\n\r Download pre-requisites  $ sudo apt-get update \u0026amp;\u0026amp; sudo apt-get install -y apt-transport-https curl   Add gpg key for apt  $ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg |sudo apt-key add -   Add apt repository  $ cat \u0026lt;\u0026lt;EOF |sudo tee -a /etc/apt/sources.list.d/kubernetes.list deb https://apt.kubernetes.io/ kubernetes-xenial main EOF   Install kubelet , kubeadm and kubectl  $ sudo apt-get update $ sudo apt-get install -y kubelet kubeadm kubectl $ sudo apt-mark hold kubelet kubeadm kubectl  Repeat the same steps on worker node\n"
},
{
	"uri": "/02-installation/10-master/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Deploy master Node (k8s-master-01)  Initialize kubeadm with pod IP range  $ sudo kubeadm init --apiserver-advertise-address=192.168.56.201 --pod-network-cidr=10.10.0.0/16 --service-cidr=192.168.10.0/24   Configure kubectl  $ mkdir -p $HOME/.kube $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config $ sudo chown $(id -u):$(id -g) $HOME/.kube/config   Verify master node status  $ kubectl cluster-info   Output will be like below  Kubernetes master is running at https://192.168.56.201:6443 KubeDNS is running at https://192.168.56.201:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.  Move to next session to deploy network plugin.\n\r"
},
{
	"uri": "/02-installation/11-network-plugin/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Deploy Network Plugin - Calico  Apply RBAC rules (More about RBAC will discuss later)  $ kubectl apply -f https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/rbac-kdd.yaml   Download Calico deployment YAML  $ wget https://docs.projectcalico.org/v3.3/getting-started/kubernetes/installation/hosted/kubernetes-datastore/calico-networking/1.7/calico.yaml   Edit CALICO_IPV4POOL_CIDR value to 10.10.0.0/16  - name: CALICO_IPV4POOL_CIDR value: \u0026quot;10.10.0.0/16\u0026quot;   Add name: IP_AUTODETECTION_METHOD \u0026amp; value: \u0026quot;can-reach=192.168.56.1\u0026quot; (This IP should be the host only network ip on your laptop)  ... image: quay.io/calico/node:v3.3.2 env: - name: IP_AUTODETECTION_METHOD value: \u0026quot;can-reach=192.168.56.1\u0026quot; ...   Apply Deployment  $ kubectl apply -f calico.yaml   Make sure the READY status should show same value on left and right side of / and Pod STATUS should be Running  $ kubectl get pods -n kube-system |nl  1 NAME READY STATUS RESTARTS AGE 2 calico-node-2pwv9 2/2 Running 0 20m 3 coredns-86c58d9df4-d9q2l 1/1 Running 0 21m 4 coredns-86c58d9df4-rwv7r 1/1 Running 0 21m 5 etcd-k8s-master-01 1/1 Running 0 20m 6 kube-apiserver-k8s-master-01 1/1 Running 0 20m 7 kube-controller-manager-k8s-master-01 1/1 Running 0 20m 8 kube-proxy-m6m9n 1/1 Running 0 21m 9 kube-scheduler-k8s-master-01 1/1 Running 0 20m  Contact the Trainer if the output is not the expected one after few minutes (~3-4mins).\n\r"
},
{
	"uri": "/02-installation/12-worker/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " Add worker node to cluster  Get discovery secret from Master node.  $ echo sha256:$(openssl x509 -in /etc/kubernetes/pki/ca.crt -noout -pubkey | openssl rsa -pubin -outform DER 2\u0026gt;/dev/null | sha256sum | cut -d' ' -f1)   Get node join token from Master node.  $ kubeadm token list |grep bootstra |awk '{print $1}'   Execute kubeadm command to add the Worker to cluster  $ sudo kubeadm join 192.168.56.201:6443 --token \u0026lt;token\u0026gt; --discovery-token-ca-cert-hash \u0026lt;discovery hash\u0026gt;   Verify system Pod status  $ kubectl get pods -n kube-system |nl   Output  1 NAME READY STATUS RESTARTS AGE 2 calico-node-2pwv9 2/2 Running 0 20m 3 calico-node-hwnfh 2/2 Running 0 19m 4 coredns-86c58d9df4-d9q2l 1/1 Running 0 21m 5 coredns-86c58d9df4-rwv7r 1/1 Running 0 21m 6 etcd-k8s-master-01 1/1 Running 0 20m 7 kube-apiserver-k8s-master-01 1/1 Running 0 20m 8 kube-controller-manager-k8s-master-01 1/1 Running 0 20m 9 kube-proxy-m6m9n 1/1 Running 0 21m 10 kube-proxy-shwgp 1/1 Running 0 19m 11 kube-scheduler-k8s-master-01 1/1 Running 0 20m  "
},
{
	"uri": "/15-k8s_from_sratch/",
	"title": "K8S From Scratch",
	"tags": [],
	"description": "",
	"content": " Chapter 15 K8S From Scratch We will build a kubernetes cluster from scratch\n"
},
{
	"uri": "/",
	"title": "",
	"tags": [],
	"description": "",
	"content": " History of K8S  2003-2004: Google introduced Borg system , which started as a small project to manage new search engine. Later on it was heavily used for managing internal distributed systems and jobs\n 2013: Google moved from Borg to Omega - a flexible and scalable scheduler for large clusters\n 2014: Google introduced kubernetes and big players (IBM, Docker, RedHat, Microsoft) joined the project\n 2015: Kubernetes 1.0 released and Google partnered with Linux Foundation to form the Cloud Native Computing Foundation (CNCF)\n 2016: Kubernetes went to mainstream and Helm package manager introduced and minikube was also released. Windows support added to k8s\n 2017: Kubernetes reached v.1.7 and were widely adopted by industry. IBM and Google introduced Istio service mesh.\n 2018: Industry understands the power of k8s and adoption rate increased\n 2019: Journey continues\u0026hellip;\n  "
},
{
	"uri": "/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]